{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a500f0a",
   "metadata": {},
   "source": [
    "## üîß Configuraci√≥n del Entorno\n",
    "\n",
    "Importamos librer√≠as y configuramos el entorno de trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c992b862",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Importar librer√≠as principales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2810d6bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/21/25 22:16:42] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Kedro is sending anonymous usage data with the sole purpose of improving <a href=\"file://C:\\Users\\cmuru\\OneDrive\\Escritorio\\datos_covid\\kedro-env\\Lib\\site-packages\\kedro_telemetry\\plugin.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">plugin.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\cmuru\\OneDrive\\Escritorio\\datos_covid\\kedro-env\\Lib\\site-packages\\kedro_telemetry\\plugin.py#243\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">243</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         the product. No personal data or IP addresses are stored on our side. To <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         opt out, set the `KEDRO_DISABLE_TELEMETRY` or `DO_NOT_TRACK` environment <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         variables, or create a `.telemetry` file in the current working          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         directory with the contents `consent: false`. To hide this message,      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         explicitly grant or deny consent. Read more at                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.kedro.org/en/stable/configuration/telemetry.html</span>            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/21/25 22:16:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Kedro is sending anonymous usage data with the sole purpose of improving \u001b]8;id=964027;file://C:\\Users\\cmuru\\OneDrive\\Escritorio\\datos_covid\\kedro-env\\Lib\\site-packages\\kedro_telemetry\\plugin.py\u001b\\\u001b[2mplugin.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=882085;file://C:\\Users\\cmuru\\OneDrive\\Escritorio\\datos_covid\\kedro-env\\Lib\\site-packages\\kedro_telemetry\\plugin.py#243\u001b\\\u001b[2m243\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         the product. No personal data or IP addresses are stored on our side. To \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         opt out, set the `KEDRO_DISABLE_TELEMETRY` or `DO_NOT_TRACK` environment \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         variables, or create a `.telemetry` file in the current working          \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         directory with the contents `consent: false`. To hide this message,      \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         explicitly grant or deny consent. Read more at                           \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://docs.kedro.org/en/stable/configuration/telemetry.html\u001b[0m            \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Contexto Kedro cargado exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Cargar contexto de Kedro\n",
    "try:\n",
    "    import os\n",
    "    from kedro.framework.session import KedroSession\n",
    "    from kedro.framework.startup import bootstrap_project\n",
    "\n",
    "    # Cambiar al directorio ra√≠z del proyecto\n",
    "    original_dir = os.getcwd()\n",
    "    project_path = os.path.dirname(os.getcwd())\n",
    "    os.chdir(project_path)\n",
    "    \n",
    "    bootstrap_project(project_path)\n",
    "    session = KedroSession.create()\n",
    "    context = session.load_context()\n",
    "    catalog = context.catalog\n",
    "    \n",
    "    print(\"‚úÖ Contexto Kedro cargado exitosamente\")\n",
    "    os.chdir(original_dir)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error cargando Kedro: {e}\")\n",
    "    print(\"Continuando sin contexto Kedro...\")\n",
    "    catalog = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a26270",
   "metadata": {},
   "source": [
    "## üìä 3.1 Carga de Datos\n",
    "\n",
    "Cargamos los datos procesados de las fases anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdbf8708",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/21/25 22:16:48] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading data from <span style=\"color: #ff8700; text-decoration-color: #ff8700\">primary_covid_complete</span> <span style=\"font-weight: bold\">(</span>CSVDataset<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>          <a href=\"file://C:\\Users\\cmuru\\OneDrive\\Escritorio\\datos_covid\\kedro-env\\Lib\\site-packages\\kedro\\io\\data_catalog.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_catalog.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\cmuru\\OneDrive\\Escritorio\\datos_covid\\kedro-env\\Lib\\site-packages\\kedro\\io\\data_catalog.py#1046\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1046</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/21/25 22:16:48]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading data from \u001b[38;5;208mprimary_covid_complete\u001b[0m \u001b[1m(\u001b[0mCSVDataset\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m          \u001b]8;id=911989;file://C:\\Users\\cmuru\\OneDrive\\Escritorio\\datos_covid\\kedro-env\\Lib\\site-packages\\kedro\\io\\data_catalog.py\u001b\\\u001b[2mdata_catalog.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=855980;file://C:\\Users\\cmuru\\OneDrive\\Escritorio\\datos_covid\\kedro-env\\Lib\\site-packages\\kedro\\io\\data_catalog.py#1046\u001b\\\u001b[2m1046\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos cargados desde Kedro: (99193, 15)\n",
      "\n",
      "üìä Dataset de trabajo:\n",
      "   ‚Ä¢ Forma: (99193, 15)\n",
      "   ‚Ä¢ Columnas: ['location_key', 'new_recovered', 'year', 'new_deceased', 'cumulative_recovered', 'new_tested', 'date', 'cumulative_confirmed', 'new_confirmed', 'cumulative_tested', 'cumulative_deceased', 'month', 'quarter', 'day_of_week', 'week_of_year']\n",
      "   ‚Ä¢ Rango temporal: 2020-01-01 00:00:00 a 2022-09-13 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos desde cat√°logo Kedro o crear datos de ejemplo\n",
    "if catalog is not None:\n",
    "    try:\n",
    "        # Intentar cargar datos procesados\n",
    "        df_trabajo = catalog.load(\"primary_covid_complete\")\n",
    "        print(f\"‚úÖ Datos cargados desde Kedro: {df_trabajo.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error cargando datos procesados: {e}\")\n",
    "        try:\n",
    "            # Fallback: cargar datos raw\n",
    "            df_2020 = catalog.load(\"raw_covid_2020\")\n",
    "            df_2021 = catalog.load(\"raw_covid_2021\") \n",
    "            df_2022 = catalog.load(\"raw_covid_2022\")\n",
    "            df_trabajo = pd.concat([df_2020, df_2021, df_2022], ignore_index=True)\n",
    "            print(f\"‚úÖ Datos raw concatenados: {df_trabajo.shape}\")\n",
    "        except:\n",
    "            df_trabajo = None\n",
    "else:\n",
    "    df_trabajo = None\n",
    "\n",
    "# Crear datos de ejemplo si no hay datos disponibles\n",
    "if df_trabajo is None or df_trabajo.empty:\n",
    "    print(\"‚ö†Ô∏è Creando datos de ejemplo para demostraci√≥n...\")\n",
    "    \n",
    "    # Generar datos sint√©ticos para COVID-19 Chile\n",
    "    dates = pd.date_range('2020-01-01', '2022-12-31', freq='D')\n",
    "    n_days = len(dates)\n",
    "    \n",
    "    # Simular datos realistas de COVID-19\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Casos nuevos con tendencias estacionales\n",
    "    base_cases = 50 + 200 * np.sin(np.arange(n_days) * 2 * np.pi / 365.25) + np.random.exponential(30, n_days)\n",
    "    # Agregar olas pand√©micas\n",
    "    wave1 = np.where((dates >= '2020-06-01') & (dates <= '2020-09-01'), \n",
    "                     np.random.exponential(100, n_days), 0)\n",
    "    wave2 = np.where((dates >= '2021-03-01') & (dates <= '2021-06-01'), \n",
    "                     np.random.exponential(150, n_days), 0)\n",
    "    \n",
    "    new_confirmed = np.maximum(0, base_cases + wave1 + wave2).astype(int)\n",
    "    cumulative_confirmed = np.cumsum(new_confirmed)\n",
    "    \n",
    "    # Muertes (aproximadamente 2% de casos con retraso)\n",
    "    new_deceased = np.maximum(0, np.random.poisson(new_confirmed * 0.02, n_days))\n",
    "    cumulative_deceased = np.cumsum(new_deceased)\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    df_trabajo = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'location_key': 'CL',\n",
    "        'new_confirmed': new_confirmed,\n",
    "        'cumulative_confirmed': cumulative_confirmed,\n",
    "        'new_deceased': new_deceased,\n",
    "        'cumulative_deceased': cumulative_deceased,\n",
    "        'population': 19116201,  # Poblaci√≥n Chile aproximada\n",
    "        'region': 'Nacional'\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ Datos sint√©ticos creados: {df_trabajo.shape}\")\n",
    "\n",
    "# Verificar datos\n",
    "print(f\"\\nüìä Dataset de trabajo:\")\n",
    "print(f\"   ‚Ä¢ Forma: {df_trabajo.shape}\")\n",
    "print(f\"   ‚Ä¢ Columnas: {list(df_trabajo.columns)}\")\n",
    "print(f\"   ‚Ä¢ Rango temporal: {df_trabajo['date'].min()} a {df_trabajo['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460e578",
   "metadata": {},
   "source": [
    "## üßπ 3.2 Limpieza de Datos\n",
    "\n",
    "### 3.2.1 An√°lisis y Tratamiento de Valores Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af2ac9f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç AN√ÅLISIS DE VALORES FALTANTES\n",
      "==================================================\n",
      "üìä Resumen general:\n",
      "   ‚Ä¢ Total registros: 99,193\n",
      "   ‚Ä¢ Columnas con datos faltantes: 0\n",
      "‚úÖ No se encontraron valores faltantes\n"
     ]
    }
   ],
   "source": [
    "def analizar_valores_faltantes(df):\n",
    "    \"\"\"\n",
    "    Analiza patrones de valores faltantes en detalle.\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç AN√ÅLISIS DE VALORES FALTANTES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"‚ùå Dataset vac√≠o\")\n",
    "        return {}\n",
    "    \n",
    "    # An√°lisis de completitud\n",
    "    missing_info = {}\n",
    "    missing_count = df.isnull().sum()\n",
    "    missing_percent = (missing_count / len(df)) * 100\n",
    "    \n",
    "    print(f\"üìä Resumen general:\")\n",
    "    print(f\"   ‚Ä¢ Total registros: {len(df):,}\")\n",
    "    print(f\"   ‚Ä¢ Columnas con datos faltantes: {(missing_count > 0).sum()}\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if missing_count[col] > 0:\n",
    "            pct = missing_percent[col]\n",
    "            \n",
    "            # Clasificar severidad\n",
    "            if pct >= 50:\n",
    "                nivel = \"üî¥ CR√çTICO\"\n",
    "                estrategia = \"Considerar eliminar columna\"\n",
    "            elif pct >= 20:\n",
    "                nivel = \"üü° ALTO\"\n",
    "                estrategia = \"Imputaci√≥n avanzada necesaria\"\n",
    "            elif pct >= 5:\n",
    "                nivel = \"üü† MODERADO\"\n",
    "                estrategia = \"Imputaci√≥n est√°ndar\"\n",
    "            else:\n",
    "                nivel = \"üü¢ BAJO\"\n",
    "                estrategia = \"Imputaci√≥n simple\"\n",
    "            \n",
    "            missing_info[col] = {\n",
    "                'count': int(missing_count[col]),\n",
    "                'percent': pct,\n",
    "                'nivel': nivel,\n",
    "                'estrategia_recomendada': estrategia\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚Ä¢ {col}: {missing_count[col]} ({pct:.1f}%) - {nivel}\")\n",
    "    \n",
    "    if not missing_info:\n",
    "        print(\"‚úÖ No se encontraron valores faltantes\")\n",
    "    \n",
    "    return missing_info\n",
    "\n",
    "# Analizar valores faltantes\n",
    "missing_analysis = analizar_valores_faltantes(df_trabajo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7bff129",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß APLICACI√ìN DE ESTRATEGIAS DE IMPUTACI√ìN\n",
      "==================================================\n",
      "‚úÖ No hay valores faltantes que tratar\n"
     ]
    }
   ],
   "source": [
    "def aplicar_estrategias_imputacion(df, missing_info):\n",
    "    \"\"\"\n",
    "    Aplica estrategias diferenciadas de imputaci√≥n seg√∫n el tipo de variable.\n",
    "    \"\"\"\n",
    "    print(\"\\nüîß APLICACI√ìN DE ESTRATEGIAS DE IMPUTACI√ìN\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_imputado = df.copy()\n",
    "    estrategias_aplicadas = {}\n",
    "    \n",
    "    if not missing_info:\n",
    "        print(\"‚úÖ No hay valores faltantes que tratar\")\n",
    "        return df_imputado, estrategias_aplicadas\n",
    "    \n",
    "    for col, info in missing_info.items():\n",
    "        if col not in df_imputado.columns:\n",
    "            continue\n",
    "            \n",
    "        pct_missing = info['percent']\n",
    "        print(f\"\\nüìä Tratando columna: {col} ({pct_missing:.1f}% faltante)\")\n",
    "        \n",
    "        # Determinar estrategia seg√∫n tipo de variable y porcentaje faltante\n",
    "        if pct_missing >= 50:\n",
    "            print(f\"   ‚ö†Ô∏è Demasiados valores faltantes. Marcando para eliminaci√≥n.\")\n",
    "            estrategias_aplicadas[col] = \"ELIMINAR\"\n",
    "            continue\n",
    "        \n",
    "        # Identificar tipo de variable\n",
    "        if df_imputado[col].dtype in ['object', 'category']:\n",
    "            # Variables categ√≥ricas\n",
    "            estrategia = \"moda\"\n",
    "            valor_imputacion = df_imputado[col].mode()[0] if len(df_imputado[col].mode()) > 0 else \"MISSING\"\n",
    "            df_imputado[col] = df_imputado[col].fillna(valor_imputacion)\n",
    "            estrategias_aplicadas[col] = f\"Imputaci√≥n por moda: {valor_imputacion}\"\n",
    "            \n",
    "        elif 'date' in col.lower():\n",
    "            # Variables de fecha\n",
    "            estrategia = \"interpolaci√≥n_temporal\"\n",
    "            df_imputado[col] = pd.to_datetime(df_imputado[col])\n",
    "            df_imputado[col] = df_imputado[col].interpolate(method='time')\n",
    "            estrategias_aplicadas[col] = \"Interpolaci√≥n temporal\"\n",
    "            \n",
    "        else:\n",
    "            # Variables num√©ricas\n",
    "            if pct_missing < 5:\n",
    "                # Imputaci√≥n simple por mediana\n",
    "                estrategia = \"mediana\"\n",
    "                valor_mediana = df_imputado[col].median()\n",
    "                df_imputado[col] = df_imputado[col].fillna(valor_mediana)\n",
    "                estrategias_aplicadas[col] = f\"Imputaci√≥n por mediana: {valor_mediana:.2f}\"\n",
    "                \n",
    "            elif pct_missing < 20:\n",
    "                # Imputaci√≥n por media si es distribuci√≥n normal\n",
    "                if abs(df_imputado[col].skew()) < 1:  # Distribuci√≥n aproximadamente normal\n",
    "                    estrategia = \"media\"\n",
    "                    valor_media = df_imputado[col].mean()\n",
    "                    df_imputado[col] = df_imputado[col].fillna(valor_media)\n",
    "                    estrategias_aplicadas[col] = f\"Imputaci√≥n por media: {valor_media:.2f}\"\n",
    "                else:\n",
    "                    # Usar mediana para distribuciones asim√©tricas\n",
    "                    estrategia = \"mediana\"\n",
    "                    valor_mediana = df_imputado[col].median()\n",
    "                    df_imputado[col] = df_imputado[col].fillna(valor_mediana)\n",
    "                    estrategias_aplicadas[col] = f\"Imputaci√≥n por mediana: {valor_mediana:.2f}\"\n",
    "            else:\n",
    "                # Imputaci√≥n avanzada con KNN\n",
    "                try:\n",
    "                    estrategia = \"KNN\"\n",
    "                    # Preparar datos para KNN (solo variables num√©ricas)\n",
    "                    numeric_cols = df_imputado.select_dtypes(include=[np.number]).columns\n",
    "                    df_numeric = df_imputado[numeric_cols].copy()\n",
    "                    \n",
    "                    # Aplicar KNN imputer\n",
    "                    imputer = KNNImputer(n_neighbors=5)\n",
    "                    df_numeric_imputed = pd.DataFrame(\n",
    "                        imputer.fit_transform(df_numeric),\n",
    "                        columns=df_numeric.columns,\n",
    "                        index=df_numeric.index\n",
    "                    )\n",
    "                    \n",
    "                    df_imputado[col] = df_numeric_imputed[col]\n",
    "                    estrategias_aplicadas[col] = \"Imputaci√≥n KNN (k=5)\"\n",
    "                except Exception as e:\n",
    "                    # Fallback a mediana\n",
    "                    print(f\"   ‚ö†Ô∏è Error en KNN, usando mediana: {e}\")\n",
    "                    valor_mediana = df_imputado[col].median()\n",
    "                    df_imputado[col] = df_imputado[col].fillna(valor_mediana)\n",
    "                    estrategias_aplicadas[col] = f\"Fallback mediana: {valor_mediana:.2f}\"\n",
    "        \n",
    "        print(f\"   ‚úÖ Aplicada: {estrategias_aplicadas[col]}\")\n",
    "    \n",
    "    # Eliminar columnas marcadas para eliminaci√≥n\n",
    "    cols_eliminar = [col for col, estrategia in estrategias_aplicadas.items() if estrategia == \"ELIMINAR\"]\n",
    "    if cols_eliminar:\n",
    "        df_imputado = df_imputado.drop(columns=cols_eliminar)\n",
    "        print(f\"\\nüóëÔ∏è Columnas eliminadas: {cols_eliminar}\")\n",
    "    \n",
    "    print(f\"\\nüìä Resultado de imputaci√≥n:\")\n",
    "    print(f\"   ‚Ä¢ Dataset original: {df.shape}\")\n",
    "    print(f\"   ‚Ä¢ Dataset imputado: {df_imputado.shape}\")\n",
    "    print(f\"   ‚Ä¢ Estrategias aplicadas: {len(estrategias_aplicadas)}\")\n",
    "    \n",
    "    return df_imputado, estrategias_aplicadas\n",
    "\n",
    "# Aplicar estrategias de imputaci√≥n\n",
    "df_trabajo_limpio, estrategias = aplicar_estrategias_imputacion(df_trabajo, missing_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092837e8",
   "metadata": {},
   "source": [
    "### 3.2.2 Detecci√≥n y Tratamiento de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "609b60e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ DETECCI√ìN Y TRATAMIENTO DE OUTLIERS\n",
      "==================================================\n",
      "üìä Variables a analizar: 11\n",
      "\n",
      "üìà Analizando: new_recovered\n",
      "   ‚Ä¢ Outliers detectados: 77 (0.1%)\n",
      "   ‚úÖ Tratamiento: Eliminaci√≥n directa (77 valores)\n",
      "\n",
      "üìà Analizando: new_deceased\n",
      "   ‚Ä¢ Outliers detectados: 8986 (9.1%)\n",
      "   ‚úÖ Tratamiento: Winsorizaci√≥n (1%-99%)\n",
      "\n",
      "üìà Analizando: cumulative_recovered\n",
      "   ‚Ä¢ Outliers detectados: 77 (0.1%)\n",
      "   ‚úÖ Tratamiento: Eliminaci√≥n directa (77 valores)\n",
      "\n",
      "üìà Analizando: new_tested\n",
      "   ‚Ä¢ Outliers detectados: 8738 (8.8%)\n",
      "   ‚úÖ Tratamiento: Winsorizaci√≥n (1%-99%)\n",
      "\n",
      "üìà Analizando: cumulative_confirmed\n",
      "   ‚Ä¢ Outliers detectados: 14533 (14.7%)\n",
      "   ‚úÖ Tratamiento: Winsorizaci√≥n (1%-99%)\n",
      "\n",
      "üìä Resumen tratamiento outliers:\n",
      "   ‚Ä¢ Variables analizadas: 5\n",
      "   ‚Ä¢ Variables tratadas: 5\n"
     ]
    }
   ],
   "source": [
    "def detectar_y_tratar_outliers(df):\n",
    "    \"\"\"\n",
    "    Detecta outliers usando m√∫ltiples m√©todos y aplica tratamiento diferenciado.\n",
    "    \"\"\"\n",
    "    print(\"\\nüéØ DETECCI√ìN Y TRATAMIENTO DE OUTLIERS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_outliers = df.copy()\n",
    "    outliers_treatment = {}\n",
    "    \n",
    "    # Variables num√©ricas para an√°lisis\n",
    "    vars_numericas = df_outliers.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    # Excluir variables que no deben tratarse como outliers\n",
    "    vars_excluir = ['year', 'month', 'day', 'population']\n",
    "    vars_analizar = [var for var in vars_numericas if var not in vars_excluir]\n",
    "    \n",
    "    print(f\"üìä Variables a analizar: {len(vars_analizar)}\")\n",
    "    \n",
    "    for var in vars_analizar[:5]:  # Limitar para demostraci√≥n\n",
    "        if var not in df_outliers.columns:\n",
    "            continue\n",
    "            \n",
    "        serie_original = df_outliers[var].dropna()\n",
    "        if len(serie_original) < 10:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüìà Analizando: {var}\")\n",
    "        \n",
    "        # M√©todo 1: IQR\n",
    "        Q1 = serie_original.quantile(0.25)\n",
    "        Q3 = serie_original.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        limite_inferior = Q1 - 1.5 * IQR\n",
    "        limite_superior = Q3 + 1.5 * IQR\n",
    "        outliers_iqr = (serie_original < limite_inferior) | (serie_original > limite_superior)\n",
    "        \n",
    "        # M√©todo 2: Z-Score\n",
    "        z_scores = np.abs(stats.zscore(serie_original))\n",
    "        outliers_zscore = z_scores > 3\n",
    "        \n",
    "        # Combinar m√©todos\n",
    "        outliers_combinados = outliers_iqr | outliers_zscore\n",
    "        pct_outliers = outliers_combinados.sum() / len(serie_original) * 100\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Outliers detectados: {outliers_combinados.sum()} ({pct_outliers:.1f}%)\")\n",
    "        \n",
    "        # Estrategia de tratamiento seg√∫n porcentaje\n",
    "        if pct_outliers <= 1:\n",
    "            # Pocos outliers: eliminar\n",
    "            mask_clean = ~outliers_combinados\n",
    "            valores_limpios = serie_original[mask_clean]\n",
    "            df_outliers.loc[df_outliers[var].notna(), var] = valores_limpios.reindex(\n",
    "                df_outliers.loc[df_outliers[var].notna()].index, fill_value=np.nan)\n",
    "            outliers_treatment[var] = f\"Eliminaci√≥n directa ({outliers_combinados.sum()} valores)\"\n",
    "            \n",
    "        elif pct_outliers <= 5:\n",
    "            # Moderados outliers: winsorizaci√≥n\n",
    "            limite_inf_wins = serie_original.quantile(0.05)\n",
    "            limite_sup_wins = serie_original.quantile(0.95)\n",
    "            df_outliers[var] = df_outliers[var].clip(lower=limite_inf_wins, upper=limite_sup_wins)\n",
    "            outliers_treatment[var] = f\"Winsorizaci√≥n (5%-95%)\"\n",
    "            \n",
    "        elif pct_outliers <= 15:\n",
    "            # Muchos outliers: transformaci√≥n log\n",
    "            if (serie_original > 0).all():\n",
    "                df_outliers[var] = np.log1p(df_outliers[var])\n",
    "                outliers_treatment[var] = f\"Transformaci√≥n logar√≠tmica\"\n",
    "            else:\n",
    "                # Si hay valores <= 0, usar winsorizaci√≥n\n",
    "                limite_inf_wins = serie_original.quantile(0.01)\n",
    "                limite_sup_wins = serie_original.quantile(0.99)\n",
    "                df_outliers[var] = df_outliers[var].clip(lower=limite_inf_wins, upper=limite_sup_wins)\n",
    "                outliers_treatment[var] = f\"Winsorizaci√≥n (1%-99%)\"\n",
    "        else:\n",
    "            # Demasiados outliers: solo winsorizaci√≥n suave\n",
    "            limite_inf_wins = serie_original.quantile(0.01)\n",
    "            limite_sup_wins = serie_original.quantile(0.99)\n",
    "            df_outliers[var] = df_outliers[var].clip(lower=limite_inf_wins, upper=limite_sup_wins)\n",
    "            outliers_treatment[var] = f\"Winsorizaci√≥n suave (1%-99%)\"\n",
    "        \n",
    "        print(f\"   ‚úÖ Tratamiento: {outliers_treatment[var]}\")\n",
    "    \n",
    "    print(f\"\\nüìä Resumen tratamiento outliers:\")\n",
    "    print(f\"   ‚Ä¢ Variables analizadas: {len(vars_analizar[:5])}\")\n",
    "    print(f\"   ‚Ä¢ Variables tratadas: {len(outliers_treatment)}\")\n",
    "    \n",
    "    return df_outliers, outliers_treatment\n",
    "\n",
    "# Detectar y tratar outliers\n",
    "df_trabajo_limpio, outliers_treatment = detectar_y_tratar_outliers(df_trabajo_limpio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac3320",
   "metadata": {},
   "source": [
    "## üîß 3.3 Feature Engineering Avanzado\n",
    "\n",
    "### 3.3.1 Variables Temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8698f751",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è∞ CREACI√ìN DE FEATURES TEMPORALES\n",
      "==================================================\n",
      "‚úÖ Features temporales creadas: 16 nuevas variables\n",
      "   ‚Ä¢ B√°sicas: year, month, day, day_of_week, day_of_year, week_of_year, quarter\n",
      "   ‚Ä¢ C√≠clicas: month_sin/cos, day_of_week_sin/cos\n",
      "   ‚Ä¢ Especiales: is_weekend, is_month_start/end, is_quarter_start\n",
      "   ‚Ä¢ Pandemia: days_since_pandemic_start, pandemic_period\n"
     ]
    }
   ],
   "source": [
    "def crear_features_temporales(df):\n",
    "    \"\"\"\n",
    "    Crea features avanzadas basadas en informaci√≥n temporal.\n",
    "    \"\"\"\n",
    "    print(\"\\n‚è∞ CREACI√ìN DE FEATURES TEMPORALES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_temporal = df.copy()\n",
    "    \n",
    "    if 'date' in df_temporal.columns:\n",
    "        # Asegurar que date es datetime\n",
    "        df_temporal['date'] = pd.to_datetime(df_temporal['date'])\n",
    "        \n",
    "        # Features b√°sicas de fecha\n",
    "        df_temporal['year'] = df_temporal['date'].dt.year\n",
    "        df_temporal['month'] = df_temporal['date'].dt.month\n",
    "        df_temporal['day'] = df_temporal['date'].dt.day\n",
    "        df_temporal['day_of_week'] = df_temporal['date'].dt.dayofweek  # 0=Monday\n",
    "        df_temporal['day_of_year'] = df_temporal['date'].dt.dayofyear\n",
    "        df_temporal['week_of_year'] = df_temporal['date'].dt.isocalendar().week\n",
    "        df_temporal['quarter'] = df_temporal['date'].dt.quarter\n",
    "        \n",
    "        # Features c√≠clicas (para capturar naturaleza c√≠clica del tiempo)\n",
    "        df_temporal['month_sin'] = np.sin(2 * np.pi * df_temporal['month'] / 12)\n",
    "        df_temporal['month_cos'] = np.cos(2 * np.pi * df_temporal['month'] / 12)\n",
    "        df_temporal['day_of_week_sin'] = np.sin(2 * np.pi * df_temporal['day_of_week'] / 7)\n",
    "        df_temporal['day_of_week_cos'] = np.cos(2 * np.pi * df_temporal['day_of_week'] / 7)\n",
    "        \n",
    "        # Features especiales\n",
    "        df_temporal['is_weekend'] = (df_temporal['day_of_week'] >= 5).astype(int)\n",
    "        df_temporal['is_month_start'] = (df_temporal['day'] <= 7).astype(int)\n",
    "        df_temporal['is_month_end'] = (df_temporal['day'] >= 25).astype(int)\n",
    "        df_temporal['is_quarter_start'] = ((df_temporal['month'] % 3 == 1) & (df_temporal['day'] <= 7)).astype(int)\n",
    "        \n",
    "        # D√≠as desde el inicio de la pandemia\n",
    "        fecha_inicio_pandemia = pd.to_datetime('2020-03-01')  # Aproximado para Chile\n",
    "        df_temporal['days_since_pandemic_start'] = (df_temporal['date'] - fecha_inicio_pandemia).dt.days\n",
    "        \n",
    "        # Per√≠odo de la pandemia\n",
    "        def asignar_periodo_pandemia(fecha):\n",
    "            if fecha < pd.to_datetime('2020-06-01'):\n",
    "                return 'inicial'\n",
    "            elif fecha < pd.to_datetime('2021-01-01'):\n",
    "                return 'primera_ola'\n",
    "            elif fecha < pd.to_datetime('2021-07-01'):\n",
    "                return 'vacunacion_inicial'\n",
    "            elif fecha < pd.to_datetime('2022-01-01'):\n",
    "                return 'segunda_ola'\n",
    "            else:\n",
    "                return 'endemica'\n",
    "        \n",
    "        df_temporal['pandemic_period'] = df_temporal['date'].apply(asignar_periodo_pandemia)\n",
    "        \n",
    "        print(f\"‚úÖ Features temporales creadas: 16 nuevas variables\")\n",
    "        print(f\"   ‚Ä¢ B√°sicas: year, month, day, day_of_week, day_of_year, week_of_year, quarter\")\n",
    "        print(f\"   ‚Ä¢ C√≠clicas: month_sin/cos, day_of_week_sin/cos\")\n",
    "        print(f\"   ‚Ä¢ Especiales: is_weekend, is_month_start/end, is_quarter_start\")\n",
    "        print(f\"   ‚Ä¢ Pandemia: days_since_pandemic_start, pandemic_period\")\n",
    "        \n",
    "        return df_temporal\n",
    "    else:\n",
    "        print(\"‚ùå No se encontr√≥ columna 'date' para features temporales\")\n",
    "        return df_temporal\n",
    "\n",
    "# Crear features temporales\n",
    "df_con_features_temporales = crear_features_temporales(df_trabajo_limpio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16a2a51",
   "metadata": {},
   "source": [
    "### 3.3.2 Variables de Tendencias y Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "025a92cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà CREACI√ìN DE FEATURES DE TENDENCIAS Y LAGS\n",
      "==================================================\n",
      "\n",
      "üìä Procesando variable: new_recovered\n",
      "   ‚úÖ 26 features creadas\n",
      "\n",
      "üìä Procesando variable: new_deceased\n",
      "   ‚úÖ 26 features creadas\n",
      "\n",
      "üìä Procesando variable: cumulative_recovered\n",
      "   ‚úÖ 26 features creadas\n",
      "\n",
      "üìä Procesando variable: new_tested\n",
      "   ‚úÖ 26 features creadas\n",
      "\n",
      "üìä Procesando variable: cumulative_confirmed\n",
      "   ‚úÖ 26 features creadas\n",
      "\n",
      "üìä Procesando variable: new_confirmed\n",
      "   ‚úÖ 26 features creadas\n",
      "\n",
      "üìä Procesando variable: cumulative_tested\n",
      "   ‚úÖ 26 features creadas\n",
      "\n",
      "üìä Procesando variable: cumulative_deceased\n",
      "   ‚úÖ 26 features creadas\n",
      "\n",
      "üìä Total de features de tendencias creadas: 208\n"
     ]
    }
   ],
   "source": [
    "def crear_features_tendencias_lags(df):\n",
    "    \"\"\"\n",
    "    Crea features de tendencias, lags y rolling statistics.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìà CREACI√ìN DE FEATURES DE TENDENCIAS Y LAGS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_trends = df.copy()\n",
    "    \n",
    "    # Variables num√©ricas para an√°lisis de tendencias\n",
    "    vars_numericas = df_trends.select_dtypes(include=[np.number]).columns\n",
    "    vars_covid = [col for col in vars_numericas if any(keyword in col.lower() \n",
    "                  for keyword in ['confirmed', 'deceased', 'recovered', 'tested'])]\n",
    "    \n",
    "    if len(vars_covid) > 0 and 'date' in df_trends.columns:\n",
    "        # Ordenar por fecha\n",
    "        df_trends = df_trends.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        features_creadas = []\n",
    "        \n",
    "        for var in vars_covid:\n",
    "            if var in df_trends.columns:\n",
    "                print(f\"\\nüìä Procesando variable: {var}\")\n",
    "                \n",
    "                # 1. Variables de Lag (valores pasados)\n",
    "                for lag in [1, 3, 7, 14]:\n",
    "                    lag_col = f\"{var}_lag_{lag}\"\n",
    "                    df_trends[lag_col] = df_trends[var].shift(lag)\n",
    "                    features_creadas.append(lag_col)\n",
    "                \n",
    "                # 2. Rolling Statistics (ventanas m√≥viles)\n",
    "                for window in [3, 7, 14, 30]:\n",
    "                    # Media m√≥vil\n",
    "                    rolling_mean_col = f\"{var}_rolling_mean_{window}\"\n",
    "                    df_trends[rolling_mean_col] = df_trends[var].rolling(window=window, min_periods=1).mean()\n",
    "                    features_creadas.append(rolling_mean_col)\n",
    "                    \n",
    "                    # Desviaci√≥n est√°ndar m√≥vil\n",
    "                    rolling_std_col = f\"{var}_rolling_std_{window}\"\n",
    "                    df_trends[rolling_std_col] = df_trends[var].rolling(window=window, min_periods=1).std()\n",
    "                    features_creadas.append(rolling_std_col)\n",
    "                    \n",
    "                    # M√°ximo y m√≠nimo m√≥vil\n",
    "                    rolling_max_col = f\"{var}_rolling_max_{window}\"\n",
    "                    rolling_min_col = f\"{var}_rolling_min_{window}\"\n",
    "                    df_trends[rolling_max_col] = df_trends[var].rolling(window=window, min_periods=1).max()\n",
    "                    df_trends[rolling_min_col] = df_trends[var].rolling(window=window, min_periods=1).min()\n",
    "                    features_creadas.extend([rolling_max_col, rolling_min_col])\n",
    "                \n",
    "                # 3. Variables de Cambio (diferencias y tasas)\n",
    "                diff_1_col = f\"{var}_diff_1\"\n",
    "                diff_7_col = f\"{var}_diff_7\"\n",
    "                df_trends[diff_1_col] = df_trends[var].diff(1)\n",
    "                df_trends[diff_7_col] = df_trends[var].diff(7)\n",
    "                features_creadas.extend([diff_1_col, diff_7_col])\n",
    "                \n",
    "                # Tasa de cambio porcentual\n",
    "                pct_change_1_col = f\"{var}_pct_change_1\"\n",
    "                pct_change_7_col = f\"{var}_pct_change_7\"\n",
    "                df_trends[pct_change_1_col] = df_trends[var].pct_change(1)\n",
    "                df_trends[pct_change_7_col] = df_trends[var].pct_change(7)\n",
    "                features_creadas.extend([pct_change_1_col, pct_change_7_col])\n",
    "                \n",
    "                # 4. Variables de Aceleraci√≥n (segunda derivada)\n",
    "                acceleration_col = f\"{var}_acceleration\"\n",
    "                df_trends[acceleration_col] = df_trends[diff_1_col].diff(1)\n",
    "                features_creadas.append(acceleration_col)\n",
    "                \n",
    "                # 5. Variables de Posici√≥n Relativa\n",
    "                relative_pos_col = f\"{var}_relative_position_30d\"\n",
    "                rolling_min_30 = df_trends[var].rolling(window=30, min_periods=1).min()\n",
    "                rolling_max_30 = df_trends[var].rolling(window=30, min_periods=1).max()\n",
    "                df_trends[relative_pos_col] = ((df_trends[var] - rolling_min_30) / \n",
    "                                              (rolling_max_30 - rolling_min_30 + 1e-8))\n",
    "                features_creadas.append(relative_pos_col)\n",
    "                \n",
    "                print(f\"   ‚úÖ {len([f for f in features_creadas if var in f])} features creadas\")\n",
    "        \n",
    "        print(f\"\\nüìä Total de features de tendencias creadas: {len(features_creadas)}\")\n",
    "        \n",
    "        return df_trends, features_creadas\n",
    "    else:\n",
    "        print(\"‚ùå No se encontraron variables COVID o columna 'date'\")\n",
    "        return df_trends, []\n",
    "\n",
    "# Crear features de tendencias y lags\n",
    "df_con_trends, features_trends = crear_features_tendencias_lags(df_con_features_temporales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30793121",
   "metadata": {},
   "source": [
    "### 3.3.3 Variables de Ratios y Proporciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d683e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¢ CREACI√ìN DE FEATURES DE RATIOS Y PROPORCIONES\n",
      "==================================================\n",
      "Variables identificadas:\n",
      "   ‚Ä¢ Casos nuevos: new_confirmed_relative_position_30d\n",
      "   ‚Ä¢ Casos acumulados: cumulative_confirmed_relative_position_30d\n",
      "   ‚Ä¢ Muertes nuevas: new_deceased_relative_position_30d\n",
      "   ‚Ä¢ Muertes acumuladas: cumulative_deceased_relative_position_30d\n",
      "\n",
      "‚úÖ Features de ratios creadas: 3\n"
     ]
    }
   ],
   "source": [
    "def crear_features_ratios(df):\n",
    "    \"\"\"\n",
    "    Crea features basadas en ratios y proporciones epidemiol√≥gicas.\n",
    "    \"\"\"\n",
    "    print(\"\\nüî¢ CREACI√ìN DE FEATURES DE RATIOS Y PROPORCIONES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_ratios = df.copy()\n",
    "    features_ratios = []\n",
    "    \n",
    "    # Encontrar variables principales\n",
    "    new_confirmed = None\n",
    "    cumulative_confirmed = None\n",
    "    new_deceased = None\n",
    "    cumulative_deceased = None\n",
    "    \n",
    "    for col in df_ratios.columns:\n",
    "        if 'new_confirmed' in col and not any(x in col for x in ['lag', 'rolling', 'diff']):\n",
    "            new_confirmed = col\n",
    "        elif 'cumulative_confirmed' in col and not any(x in col for x in ['lag', 'rolling', 'diff']):\n",
    "            cumulative_confirmed = col\n",
    "        elif 'new_deceased' in col and not any(x in col for x in ['lag', 'rolling', 'diff']):\n",
    "            new_deceased = col\n",
    "        elif 'cumulative_deceased' in col and not any(x in col for x in ['lag', 'rolling', 'diff']):\n",
    "            cumulative_deceased = col\n",
    "    \n",
    "    print(f\"Variables identificadas:\")\n",
    "    print(f\"   ‚Ä¢ Casos nuevos: {new_confirmed}\")\n",
    "    print(f\"   ‚Ä¢ Casos acumulados: {cumulative_confirmed}\")\n",
    "    print(f\"   ‚Ä¢ Muertes nuevas: {new_deceased}\")\n",
    "    print(f\"   ‚Ä¢ Muertes acumuladas: {cumulative_deceased}\")\n",
    "    \n",
    "    # 1. RATIOS EPIDEMIOL√ìGICOS B√ÅSICOS\n",
    "    if new_deceased and new_confirmed:\n",
    "        # Tasa de letalidad diaria (CFR diario)\n",
    "        cfr_daily_col = 'case_fatality_rate_daily'\n",
    "        df_ratios[cfr_daily_col] = (df_ratios[new_deceased] / \n",
    "                                   (df_ratios[new_confirmed] + 1e-8)) * 100\n",
    "        features_ratios.append(cfr_daily_col)\n",
    "    \n",
    "    if cumulative_deceased and cumulative_confirmed:\n",
    "        # Tasa de letalidad acumulada (CFR total)\n",
    "        cfr_total_col = 'case_fatality_rate_total'\n",
    "        df_ratios[cfr_total_col] = (df_ratios[cumulative_deceased] / \n",
    "                                   (df_ratios[cumulative_confirmed] + 1e-8)) * 100\n",
    "        features_ratios.append(cfr_total_col)\n",
    "    \n",
    "    # 2. RATIOS DE CRECIMIENTO\n",
    "    if new_confirmed:\n",
    "        # Ratio de casos nuevos vs promedio de la semana anterior\n",
    "        if f\"{new_confirmed}_rolling_mean_7\" in df_ratios.columns:\n",
    "            growth_ratio_col = 'weekly_growth_ratio'\n",
    "            rolling_mean_7d_lag = df_ratios[f\"{new_confirmed}_rolling_mean_7\"].shift(7)\n",
    "            df_ratios[growth_ratio_col] = (df_ratios[f\"{new_confirmed}_rolling_mean_7\"] / \n",
    "                                          (rolling_mean_7d_lag + 1e-8))\n",
    "            features_ratios.append(growth_ratio_col)\n",
    "    \n",
    "    # 3. RATIOS DE VOLATILIDAD\n",
    "    if new_confirmed:\n",
    "        if f\"{new_confirmed}_rolling_mean_7\" in df_ratios.columns and f\"{new_confirmed}_rolling_std_7\" in df_ratios.columns:\n",
    "            cv_col = 'cases_coefficient_variation_7d'\n",
    "            df_ratios[cv_col] = (df_ratios[f\"{new_confirmed}_rolling_std_7\"] / \n",
    "                                (df_ratios[f\"{new_confirmed}_rolling_mean_7\"] + 1e-8))\n",
    "            features_ratios.append(cv_col)\n",
    "    \n",
    "    # 4. RATIOS COMPARATIVOS TEMPORALES\n",
    "    if new_confirmed:\n",
    "        weekly_comparison_col = 'weekly_comparison_ratio'\n",
    "        df_ratios[weekly_comparison_col] = (df_ratios[new_confirmed] / \n",
    "                                           (df_ratios[new_confirmed].shift(7) + 1e-8))\n",
    "        features_ratios.append(weekly_comparison_col)\n",
    "    \n",
    "    # Limpiar valores infinitos y NaN\n",
    "    for col in features_ratios:\n",
    "        if col in df_ratios.columns:\n",
    "            # Reemplazar infinitos por NaN\n",
    "            df_ratios[col] = df_ratios[col].replace([np.inf, -np.inf], np.nan)\n",
    "            # Imputar valores extremos\n",
    "            if df_ratios[col].notna().sum() > 0:\n",
    "                q99 = df_ratios[col].quantile(0.99)\n",
    "                q01 = df_ratios[col].quantile(0.01)\n",
    "                df_ratios[col] = df_ratios[col].clip(lower=q01, upper=q99)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Features de ratios creadas: {len(features_ratios)}\")\n",
    "    \n",
    "    return df_ratios, features_ratios\n",
    "\n",
    "# Crear features de ratios y proporciones\n",
    "df_con_ratios, features_ratios = crear_features_ratios(df_con_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6a389",
   "metadata": {},
   "source": [
    "## üéØ 3.4 Creaci√≥n de Variables Target para ML\n",
    "\n",
    "### 3.4.1 Targets de Regresi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1714b6c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ CREACI√ìN DE TARGETS DE REGRESI√ìN\n",
      "==================================================\n",
      "‚úÖ target_cases_next_7_days: 99187 valores v√°lidos\n",
      "‚úÖ target_growth_rate_14_days: 99179 valores v√°lidos\n",
      "‚úÖ target_deaths_avg_7_days: 99187 valores v√°lidos\n",
      "‚úÖ target_volatility_14_days: 99179 valores v√°lidos\n",
      "\n",
      "üìä Resumen targets de regresi√≥n: 4 creados\n"
     ]
    }
   ],
   "source": [
    "def crear_targets_regresion(df):\n",
    "    \"\"\"\n",
    "    Crea variables target para problemas de regresi√≥n.\n",
    "    \"\"\"\n",
    "    print(\"\\nüéØ CREACI√ìN DE TARGETS DE REGRESI√ìN\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_targets = df.copy()\n",
    "    targets_regresion = {}\n",
    "    \n",
    "    # Variables base\n",
    "    new_confirmed = None\n",
    "    cumulative_confirmed = None\n",
    "    new_deceased = None\n",
    "    \n",
    "    for col in df_targets.columns:\n",
    "        if 'new_confirmed' in col and not any(x in col for x in ['lag', 'rolling', 'diff']):\n",
    "            new_confirmed = col\n",
    "        elif 'cumulative_confirmed' in col and not any(x in col for x in ['lag', 'rolling', 'diff']):\n",
    "            cumulative_confirmed = col\n",
    "        elif 'new_deceased' in col and not any(x in col for x in ['lag', 'rolling', 'diff']):\n",
    "            new_deceased = col\n",
    "    \n",
    "    # TARGET 1: Predicci√≥n de casos confirmados en pr√≥ximos 7 d√≠as\n",
    "    if new_confirmed:\n",
    "        target_name = 'target_cases_next_7_days'\n",
    "        df_targets_sorted = df_targets.sort_values('date') if 'date' in df_targets.columns else df_targets\n",
    "        casos_futuros_7d = df_targets_sorted[new_confirmed].rolling(window=7, min_periods=1).sum().shift(-6)\n",
    "        df_targets[target_name] = casos_futuros_7d\n",
    "        targets_regresion[target_name] = {\n",
    "            'descripcion': 'Suma de casos confirmados en pr√≥ximos 7 d√≠as',\n",
    "            'tipo': 'Regresi√≥n',\n",
    "            'min': casos_futuros_7d.min() if casos_futuros_7d.notna().sum() > 0 else 0,\n",
    "            'max': casos_futuros_7d.max() if casos_futuros_7d.notna().sum() > 0 else 0,\n",
    "            'mean': casos_futuros_7d.mean() if casos_futuros_7d.notna().sum() > 0 else 0,\n",
    "            'valores_validos': casos_futuros_7d.notna().sum()\n",
    "        }\n",
    "        print(f\"‚úÖ {target_name}: {casos_futuros_7d.notna().sum()} valores v√°lidos\")\n",
    "    \n",
    "    # TARGET 2: Tasa de crecimiento de casos acumulados (pr√≥ximos 14 d√≠as)\n",
    "    if cumulative_confirmed:\n",
    "        target_name = 'target_growth_rate_14_days'\n",
    "        casos_acum_futuro = df_targets[cumulative_confirmed].shift(-14)\n",
    "        tasa_crecimiento = ((casos_acum_futuro - df_targets[cumulative_confirmed]) / \n",
    "                           (df_targets[cumulative_confirmed] + 1e-8)) * 100\n",
    "        df_targets[target_name] = tasa_crecimiento\n",
    "        targets_regresion[target_name] = {\n",
    "            'descripcion': 'Tasa de crecimiento de casos acumulados en 14 d√≠as (%)',\n",
    "            'tipo': 'Regresi√≥n',\n",
    "            'min': tasa_crecimiento.min() if tasa_crecimiento.notna().sum() > 0 else 0,\n",
    "            'max': tasa_crecimiento.max() if tasa_crecimiento.notna().sum() > 0 else 0,\n",
    "            'mean': tasa_crecimiento.mean() if tasa_crecimiento.notna().sum() > 0 else 0,\n",
    "            'valores_validos': tasa_crecimiento.notna().sum()\n",
    "        }\n",
    "        print(f\"‚úÖ {target_name}: {tasa_crecimiento.notna().sum()} valores v√°lidos\")\n",
    "    \n",
    "    # TARGET 3: Promedio de muertes en pr√≥ximos 7 d√≠as\n",
    "    if new_deceased:\n",
    "        target_name = 'target_deaths_avg_7_days'\n",
    "        muertes_futuras_7d = df_targets_sorted[new_deceased].rolling(window=7, min_periods=1).mean().shift(-6)\n",
    "        df_targets[target_name] = muertes_futuras_7d\n",
    "        targets_regresion[target_name] = {\n",
    "            'descripcion': 'Promedio de muertes diarias en pr√≥ximos 7 d√≠as',\n",
    "            'tipo': 'Regresi√≥n',\n",
    "            'min': muertes_futuras_7d.min() if muertes_futuras_7d.notna().sum() > 0 else 0,\n",
    "            'max': muertes_futuras_7d.max() if muertes_futuras_7d.notna().sum() > 0 else 0,\n",
    "            'mean': muertes_futuras_7d.mean() if muertes_futuras_7d.notna().sum() > 0 else 0,\n",
    "            'valores_validos': muertes_futuras_7d.notna().sum()\n",
    "        }\n",
    "        print(f\"‚úÖ {target_name}: {muertes_futuras_7d.notna().sum()} valores v√°lidos\")\n",
    "    \n",
    "    # TARGET 4: Volatilidad futura de casos\n",
    "    if new_confirmed:\n",
    "        target_name = 'target_volatility_14_days'\n",
    "        volatilidad_futura = []\n",
    "        for i in range(len(df_targets_sorted)):\n",
    "            if i + 14 < len(df_targets_sorted):\n",
    "                casos_proximos_14 = df_targets_sorted[new_confirmed].iloc[i+1:i+15]\n",
    "                vol = casos_proximos_14.std()\n",
    "            else:\n",
    "                vol = np.nan\n",
    "            volatilidad_futura.append(vol)\n",
    "        \n",
    "        df_targets[target_name] = volatilidad_futura\n",
    "        volatilidad_series = pd.Series(volatilidad_futura)\n",
    "        targets_regresion[target_name] = {\n",
    "            'descripcion': 'Volatilidad (std) de casos en pr√≥ximos 14 d√≠as',\n",
    "            'tipo': 'Regresi√≥n',\n",
    "            'min': volatilidad_series.min() if volatilidad_series.notna().sum() > 0 else 0,\n",
    "            'max': volatilidad_series.max() if volatilidad_series.notna().sum() > 0 else 0,\n",
    "            'mean': volatilidad_series.mean() if volatilidad_series.notna().sum() > 0 else 0,\n",
    "            'valores_validos': volatilidad_series.notna().sum()\n",
    "        }\n",
    "        print(f\"‚úÖ {target_name}: {volatilidad_series.notna().sum()} valores v√°lidos\")\n",
    "    \n",
    "    print(f\"\\nüìä Resumen targets de regresi√≥n: {len(targets_regresion)} creados\")\n",
    "    return df_targets, targets_regresion\n",
    "\n",
    "# Crear targets de regresi√≥n\n",
    "df_con_targets_reg, targets_reg_info = crear_targets_regresion(df_con_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47daa61",
   "metadata": {},
   "source": [
    "### 3.4.2 Targets de Clasificaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afd6ad52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ CREACI√ìN DE TARGETS DE CLASIFICACI√ìN\n",
      "==================================================\n",
      "‚úÖ target_high_transmission_period: 99193 valores v√°lidos\n",
      "‚úÖ target_alert_level: Verde:33221, Amarillo:32480, Rojo:33492\n",
      "‚úÖ target_trend_direction: Desc:45165, Estable:4149, Asc:48813\n",
      "‚úÖ target_hospital_saturation_risk: 99187 valores v√°lidos, 5688 con riesgo\n",
      "\n",
      "üìä Resumen targets de clasificaci√≥n: 4 creados\n"
     ]
    }
   ],
   "source": [
    "def crear_targets_clasificacion(df):\n",
    "    \"\"\"\n",
    "    Crea variables target para problemas de clasificaci√≥n.\n",
    "    \"\"\"\n",
    "    print(\"\\nüéØ CREACI√ìN DE TARGETS DE CLASIFICACI√ìN\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_targets = df.copy()\n",
    "    targets_clasificacion = {}\n",
    "    \n",
    "    # Variables base\n",
    "    new_confirmed = None\n",
    "    for col in df_targets.columns:\n",
    "        if 'new_confirmed' in col and not any(x in col for x in ['lag', 'rolling', 'diff', 'target']):\n",
    "            new_confirmed = col\n",
    "            break\n",
    "    \n",
    "    # TARGET 1: Per√≠odo de Alta Transmisi√≥n (Binario)\n",
    "    if new_confirmed:\n",
    "        target_name = 'target_high_transmission_period'\n",
    "        casos_diarios = df_targets[new_confirmed].dropna()\n",
    "        if len(casos_diarios) > 0:\n",
    "            umbral_alto = casos_diarios.quantile(0.75)\n",
    "            df_targets[target_name] = (df_targets[new_confirmed] > umbral_alto).astype(int)\n",
    "            \n",
    "            valores_validos = df_targets[target_name].notna().sum()\n",
    "            casos_alto = (df_targets[target_name] == 1).sum()\n",
    "            casos_bajo = (df_targets[target_name] == 0).sum()\n",
    "            \n",
    "            targets_clasificacion[target_name] = {\n",
    "                'descripcion': f'Per√≠odo de alta transmisi√≥n (>{umbral_alto:.0f} casos/d√≠a)',\n",
    "                'tipo': 'Clasificaci√≥n Binaria',\n",
    "                'clases': {'0': 'Baja transmisi√≥n', '1': 'Alta transmisi√≥n'},\n",
    "                'distribucion': {'0': casos_bajo, '1': casos_alto},\n",
    "                'valores_validos': valores_validos\n",
    "            }\n",
    "            print(f\"‚úÖ {target_name}: {valores_validos} valores v√°lidos\")\n",
    "    \n",
    "    # TARGET 2: Nivel de Alerta Regional (Multiclase)\n",
    "    if new_confirmed:\n",
    "        target_name = 'target_alert_level'\n",
    "        casos_diarios = df_targets[new_confirmed].dropna()\n",
    "        if len(casos_diarios) > 0:\n",
    "            umbral_verde = casos_diarios.quantile(0.33)\n",
    "            umbral_amarillo = casos_diarios.quantile(0.66)\n",
    "            \n",
    "            def asignar_nivel_alerta(casos):\n",
    "                if pd.isna(casos):\n",
    "                    return np.nan\n",
    "                elif casos <= umbral_verde:\n",
    "                    return 0  # Verde\n",
    "                elif casos <= umbral_amarillo:\n",
    "                    return 1  # Amarillo\n",
    "                else:\n",
    "                    return 2  # Rojo\n",
    "            \n",
    "            df_targets[target_name] = df_targets[new_confirmed].apply(asignar_nivel_alerta)\n",
    "            \n",
    "            valores_validos = df_targets[target_name].notna().sum()\n",
    "            casos_verde = (df_targets[target_name] == 0).sum()\n",
    "            casos_amarillo = (df_targets[target_name] == 1).sum()\n",
    "            casos_rojo = (df_targets[target_name] == 2).sum()\n",
    "            \n",
    "            targets_clasificacion[target_name] = {\n",
    "                'descripcion': 'Nivel de alerta epidemiol√≥gica',\n",
    "                'tipo': 'Clasificaci√≥n Multiclase',\n",
    "                'clases': {'0': 'Verde (Bajo)', '1': 'Amarillo (Medio)', '2': 'Rojo (Alto)'},\n",
    "                'distribucion': {'0': casos_verde, '1': casos_amarillo, '2': casos_rojo},\n",
    "                'valores_validos': valores_validos\n",
    "            }\n",
    "            print(f\"‚úÖ {target_name}: Verde:{casos_verde}, Amarillo:{casos_amarillo}, Rojo:{casos_rojo}\")\n",
    "    \n",
    "    # TARGET 3: Direcci√≥n de Tendencia (Multiclase)\n",
    "    if new_confirmed and f\"{new_confirmed}_rolling_mean_7\" in df_targets.columns:\n",
    "        target_name = 'target_trend_direction'\n",
    "        rolling_mean_actual = df_targets[f\"{new_confirmed}_rolling_mean_7\"]\n",
    "        rolling_mean_anterior = rolling_mean_actual.shift(7)\n",
    "        \n",
    "        def asignar_tendencia(actual, anterior, threshold=0.05):\n",
    "            if pd.isna(actual) or pd.isna(anterior) or anterior == 0:\n",
    "                return np.nan\n",
    "            cambio_pct = (actual - anterior) / anterior\n",
    "            if cambio_pct > threshold:\n",
    "                return 2  # Ascendente\n",
    "            elif cambio_pct < -threshold:\n",
    "                return 0  # Descendente\n",
    "            else:\n",
    "                return 1  # Estable\n",
    "        \n",
    "        df_targets[target_name] = [\n",
    "            asignar_tendencia(actual, anterior) \n",
    "            for actual, anterior in zip(rolling_mean_actual, rolling_mean_anterior)\n",
    "        ]\n",
    "        \n",
    "        target_series = pd.Series(df_targets[target_name])\n",
    "        valores_validos = target_series.notna().sum()\n",
    "        casos_desc = (target_series == 0).sum()\n",
    "        casos_estable = (target_series == 1).sum()\n",
    "        casos_asc = (target_series == 2).sum()\n",
    "        \n",
    "        targets_clasificacion[target_name] = {\n",
    "            'descripcion': 'Direcci√≥n de tendencia semanal',\n",
    "            'tipo': 'Clasificaci√≥n Multiclase',\n",
    "            'clases': {'0': 'Descendente', '1': 'Estable', '2': 'Ascendente'},\n",
    "            'distribucion': {'0': casos_desc, '1': casos_estable, '2': casos_asc},\n",
    "            'valores_validos': valores_validos\n",
    "        }\n",
    "        print(f\"‚úÖ {target_name}: Desc:{casos_desc}, Estable:{casos_estable}, Asc:{casos_asc}\")\n",
    "    \n",
    "    # TARGET 4: Riesgo de Saturaci√≥n Hospitalaria (Binario)\n",
    "    if new_confirmed:\n",
    "        target_name = 'target_hospital_saturation_risk'\n",
    "        casos_diarios = df_targets[new_confirmed].dropna()\n",
    "        if len(casos_diarios) > 0:\n",
    "            umbral_saturacion = casos_diarios.quantile(0.90)\n",
    "            casos_altos = (df_targets[new_confirmed] > umbral_saturacion).astype(int)\n",
    "            \n",
    "            riesgo_saturacion = []\n",
    "            for i in range(len(casos_altos)):\n",
    "                if i >= 6:\n",
    "                    ventana = casos_altos.iloc[i-6:i+1]\n",
    "                    consecutivos = 0\n",
    "                    max_consecutivos = 0\n",
    "                    for val in ventana:\n",
    "                        if val == 1:\n",
    "                            consecutivos += 1\n",
    "                            max_consecutivos = max(max_consecutivos, consecutivos)\n",
    "                        else:\n",
    "                            consecutivos = 0\n",
    "                    riesgo_saturacion.append(1 if max_consecutivos >= 3 else 0)\n",
    "                else:\n",
    "                    riesgo_saturacion.append(np.nan)\n",
    "            \n",
    "            df_targets[target_name] = riesgo_saturacion\n",
    "            target_series = pd.Series(riesgo_saturacion)\n",
    "            valores_validos = target_series.notna().sum()\n",
    "            casos_riesgo = (target_series == 1).sum()\n",
    "            casos_sin_riesgo = (target_series == 0).sum()\n",
    "            \n",
    "            targets_clasificacion[target_name] = {\n",
    "                'descripcion': 'Riesgo de saturaci√≥n hospitalaria',\n",
    "                'tipo': 'Clasificaci√≥n Binaria',\n",
    "                'clases': {'0': 'Sin riesgo', '1': 'Con riesgo'},\n",
    "                'distribucion': {'0': casos_sin_riesgo, '1': casos_riesgo},\n",
    "                'valores_validos': valores_validos\n",
    "            }\n",
    "            print(f\"‚úÖ {target_name}: {valores_validos} valores v√°lidos, {casos_riesgo} con riesgo\")\n",
    "    \n",
    "    print(f\"\\nüìä Resumen targets de clasificaci√≥n: {len(targets_clasificacion)} creados\")\n",
    "    return df_targets, targets_clasificacion\n",
    "\n",
    "# Crear targets de clasificaci√≥n\n",
    "df_con_todos_targets, targets_class_info = crear_targets_clasificacion(df_con_targets_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde2037",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 3.5 Transformaciones y Escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbfe0bcf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öñÔ∏è APLICACI√ìN DE TRANSFORMACIONES Y ESCALADO\n",
      "============================================================\n",
      "Variables para escalado: 224\n",
      "Variables targets: 8\n",
      "Variables temporales (sin escalar): 11\n",
      "\n",
      "üìä Aplicando transformaciones logar√≠tmicas...\n",
      "   ‚úÖ Variables log transformadas: 5\n",
      "\n",
      "‚öñÔ∏è Aplicando escalado robusto...\n",
      "   ‚úÖ Escalado robusto aplicado a 8 variables\n",
      "\n",
      "üîÑ Aplicando normalizaci√≥n Min-Max...\n",
      "   ‚úÖ Normalizaci√≥n Min-Max aplicada a 13 variables\n",
      "\n",
      "üè∑Ô∏è Codificando variables categ√≥ricas...\n",
      "   ‚úÖ Label encoding: location_key (363 categor√≠as)\n",
      "   ‚úÖ One-hot encoding: pandemic_period (5 categor√≠as)\n",
      "\n",
      "üìä RESUMEN DE TRANSFORMACIONES:\n",
      "   ‚Ä¢ Dataset original: (99193, 246)\n",
      "   ‚Ä¢ Dataset transformado: (99193, 279)\n",
      "   ‚Ä¢ Variables agregadas: 33\n",
      "   ‚Ä¢ Transformaciones log: 5\n",
      "   ‚Ä¢ Variables escaladas (Robust): 8\n",
      "   ‚Ä¢ Variables normalizadas (Min-Max): 13\n",
      "   ‚Ä¢ Variables categ√≥ricas codificadas: 2\n"
     ]
    }
   ],
   "source": [
    "def aplicar_transformaciones_escalado(df, targets_reg_info, targets_class_info):\n",
    "    \"\"\"\n",
    "    Aplica transformaciones y escalado apropiados a las features.\n",
    "    \"\"\"\n",
    "    print(\"\\n‚öñÔ∏è APLICACI√ìN DE TRANSFORMACIONES Y ESCALADO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # Identificar tipos de variables\n",
    "    vars_numericas = df_scaled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    vars_targets = list(targets_reg_info.keys()) + list(targets_class_info.keys())\n",
    "    vars_features = [var for var in vars_numericas if var not in vars_targets]\n",
    "    \n",
    "    # Excluir variables temporales b√°sicas del escalado\n",
    "    vars_temporales_basicas = ['year', 'month', 'day', 'day_of_week', 'day_of_year', \n",
    "                              'week_of_year', 'quarter', 'is_weekend', 'is_month_start', \n",
    "                              'is_month_end', 'is_quarter_start']\n",
    "    vars_para_escalar = [var for var in vars_features if var not in vars_temporales_basicas]\n",
    "    \n",
    "    print(f\"Variables para escalado: {len(vars_para_escalar)}\")\n",
    "    print(f\"Variables targets: {len(vars_targets)}\")\n",
    "    print(f\"Variables temporales (sin escalar): {len([v for v in vars_temporales_basicas if v in df_scaled.columns])}\")\n",
    "    \n",
    "    # 1. TRANSFORMACIONES LOGAR√çTMICAS\n",
    "    print(f\"\\nüìä Aplicando transformaciones logar√≠tmicas...\")\n",
    "    vars_log_transformadas = []\n",
    "    \n",
    "    vars_candidatas_log = [var for var in vars_para_escalar \n",
    "                          if any(keyword in var.lower() for keyword in \n",
    "                                ['cumulative', 'confirmed', 'deceased', 'recovered', 'tested'])\n",
    "                          and not any(x in var.lower() for x in ['rate', 'ratio', 'pct', 'log'])]\n",
    "    \n",
    "    for var in vars_candidatas_log[:5]:\n",
    "        if var in df_scaled.columns:\n",
    "            serie = df_scaled[var].dropna()\n",
    "            if len(serie) > 0 and (serie >= 0).all():\n",
    "                var_log = f\"{var}_log_transform\"\n",
    "                df_scaled[var_log] = np.log1p(df_scaled[var])\n",
    "                vars_log_transformadas.append(var_log)\n",
    "                vars_para_escalar.append(var_log)\n",
    "    \n",
    "    print(f\"   ‚úÖ Variables log transformadas: {len(vars_log_transformadas)}\")\n",
    "    \n",
    "    # 2. ESCALADO ROBUSTO SIMPLIFICADO\n",
    "    print(f\"\\n‚öñÔ∏è Aplicando escalado robusto...\")\n",
    "    \n",
    "    escalador_info = {}\n",
    "    for var in vars_para_escalar[:20]:\n",
    "        if var in df_scaled.columns:\n",
    "            serie = df_scaled[var].dropna()\n",
    "            if len(serie) > 10:\n",
    "                mediana = serie.median()\n",
    "                mad = np.median(np.abs(serie - mediana))\n",
    "                \n",
    "                if mad > 1e-8:\n",
    "                    var_scaled = f\"{var}_scaled\"\n",
    "                    df_scaled[var_scaled] = (df_scaled[var] - mediana) / mad\n",
    "                    escalador_info[var] = {'mediana': mediana, 'mad': mad}\n",
    "    \n",
    "    print(f\"   ‚úÖ Escalado robusto aplicado a {len(escalador_info)} variables\")\n",
    "    \n",
    "    # 3. NORMALIZACI√ìN MIN-MAX PARA VARIABLES ESPEC√çFICAS\n",
    "    print(f\"\\nüîÑ Aplicando normalizaci√≥n Min-Max...\")\n",
    "    \n",
    "    vars_ciclicas = [var for var in df_scaled.columns if any(x in var for x in ['_sin', '_cos'])]\n",
    "    vars_ratios = [var for var in df_scaled.columns if any(x in var for x in ['rate', 'ratio'])]\n",
    "    vars_minmax = vars_ciclicas + vars_ratios[:5]\n",
    "    \n",
    "    minmax_info = {}\n",
    "    for var in vars_minmax:\n",
    "        if var in df_scaled.columns:\n",
    "            serie = df_scaled[var].dropna()\n",
    "            if len(serie) > 1:\n",
    "                min_val = serie.min()\n",
    "                max_val = serie.max()\n",
    "                \n",
    "                if max_val > min_val:\n",
    "                    var_minmax = f\"{var}_minmax\"\n",
    "                    df_scaled[var_minmax] = (df_scaled[var] - min_val) / (max_val - min_val)\n",
    "                    minmax_info[var] = {'min': min_val, 'max': max_val}\n",
    "    \n",
    "    print(f\"   ‚úÖ Normalizaci√≥n Min-Max aplicada a {len(minmax_info)} variables\")\n",
    "    \n",
    "    # 4. CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\n",
    "    print(f\"\\nüè∑Ô∏è Codificando variables categ√≥ricas...\")\n",
    "    \n",
    "    vars_categoricas = df_scaled.select_dtypes(include=['object']).columns.tolist()\n",
    "    encoding_info = {}\n",
    "    \n",
    "    for var in vars_categoricas:\n",
    "        if var in df_scaled.columns and var != 'date':\n",
    "            valores_unicos = df_scaled[var].nunique()\n",
    "            \n",
    "            if valores_unicos <= 10:\n",
    "                dummies = pd.get_dummies(df_scaled[var], prefix=var, dummy_na=True)\n",
    "                df_scaled = pd.concat([df_scaled, dummies], axis=1)\n",
    "                encoding_info[var] = f\"One-hot ({valores_unicos} categor√≠as)\"\n",
    "                print(f\"   ‚úÖ One-hot encoding: {var} ({valores_unicos} categor√≠as)\")\n",
    "            else:\n",
    "                mask_no_na = df_scaled[var].notna()\n",
    "                if mask_no_na.sum() > 0:\n",
    "                    var_encoded = f\"{var}_encoded\"\n",
    "                    df_scaled[var_encoded] = np.nan\n",
    "                    valores_unicos_lista = df_scaled.loc[mask_no_na, var].unique()\n",
    "                    mapeo = {val: i for i, val in enumerate(valores_unicos_lista)}\n",
    "                    df_scaled.loc[mask_no_na, var_encoded] = df_scaled.loc[mask_no_na, var].map(mapeo)\n",
    "                    encoding_info[var] = f\"Label encoding ({valores_unicos} categor√≠as)\"\n",
    "                    print(f\"   ‚úÖ Label encoding: {var} ({valores_unicos} categor√≠as)\")\n",
    "    \n",
    "    # Resumen final\n",
    "    print(f\"\\nüìä RESUMEN DE TRANSFORMACIONES:\")\n",
    "    print(f\"   ‚Ä¢ Dataset original: {df.shape}\")\n",
    "    print(f\"   ‚Ä¢ Dataset transformado: {df_scaled.shape}\")\n",
    "    print(f\"   ‚Ä¢ Variables agregadas: {df_scaled.shape[1] - df.shape[1]}\")\n",
    "    print(f\"   ‚Ä¢ Transformaciones log: {len(vars_log_transformadas)}\")\n",
    "    print(f\"   ‚Ä¢ Variables escaladas (Robust): {len(escalador_info)}\")\n",
    "    print(f\"   ‚Ä¢ Variables normalizadas (Min-Max): {len(minmax_info)}\")\n",
    "    print(f\"   ‚Ä¢ Variables categ√≥ricas codificadas: {len(encoding_info)}\")\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "# Aplicar transformaciones y escalado\n",
    "df_final_transformado = aplicar_transformaciones_escalado(\n",
    "    df_con_todos_targets, \n",
    "    targets_reg_info, \n",
    "    targets_class_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e9aec",
   "metadata": {},
   "source": [
    "## üìä 3.6 Divisi√≥n en Train/Validation/Test\n",
    "\n",
    "### Divisi√≥n temporal para series de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0615d902",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä DIVISI√ìN TEMPORAL DE DATOS\n",
      "==================================================\n",
      "üìà Divisi√≥n realizada:\n",
      "   ‚Ä¢ Train: 64,475 filas (65.0%)\n",
      "   ‚Ä¢ Validation: 14,879 filas (15.0%)\n",
      "   ‚Ä¢ Test: 19,839 filas (20.0%)\n",
      "\n",
      "üìÖ Rangos temporales:\n",
      "   ‚Ä¢ Train: 2020-01-01 00:00:00 a 2021-11-12 00:00:00\n",
      "   ‚Ä¢ Validation: 2021-11-12 00:00:00 a 2022-03-28 00:00:00\n",
      "   ‚Ä¢ Test: 2022-03-28 00:00:00 a 2022-09-13 00:00:00\n",
      "\n",
      "‚ö†Ô∏è Advertencia: Solapamiento temporal entre train y validation\n",
      "\n",
      "‚ö†Ô∏è Advertencia: Solapamiento temporal entre validation y test\n",
      "\n",
      "üìã Divisi√≥n guardada en variable 'division_info'\n"
     ]
    }
   ],
   "source": [
    "def dividir_datos_temporal(df, test_size=0.2, val_size=0.15):\n",
    "    \"\"\"\n",
    "    Divide los datos de manera temporal para preservar el orden cronol√≥gico.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä DIVISI√ìN TEMPORAL DE DATOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_division = df.copy()\n",
    "    \n",
    "    if 'date' in df_division.columns:\n",
    "        # Ordenar por fecha\n",
    "        df_division = df_division.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Calcular puntos de corte temporales\n",
    "        total_rows = len(df_division)\n",
    "        \n",
    "        # Test: √∫ltimos test_size% de datos\n",
    "        test_start = int(total_rows * (1 - test_size))\n",
    "        \n",
    "        # Validation: val_size% antes del test\n",
    "        val_start = int(total_rows * (1 - test_size - val_size))\n",
    "        \n",
    "        # Divisi√≥n\n",
    "        df_train = df_division.iloc[:val_start].copy()\n",
    "        df_val = df_division.iloc[val_start:test_start].copy()\n",
    "        df_test = df_division.iloc[test_start:].copy()\n",
    "        \n",
    "        # Informaci√≥n de las divisiones\n",
    "        print(f\"üìà Divisi√≥n realizada:\")\n",
    "        print(f\"   ‚Ä¢ Train: {len(df_train):,} filas ({len(df_train)/total_rows*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Validation: {len(df_val):,} filas ({len(df_val)/total_rows*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Test: {len(df_test):,} filas ({len(df_test)/total_rows*100:.1f}%)\")\n",
    "        \n",
    "        # Rangos temporales\n",
    "        if len(df_train) > 0:\n",
    "            print(f\"\\nüìÖ Rangos temporales:\")\n",
    "            print(f\"   ‚Ä¢ Train: {df_train['date'].min()} a {df_train['date'].max()}\")\n",
    "        if len(df_val) > 0:\n",
    "            print(f\"   ‚Ä¢ Validation: {df_val['date'].min()} a {df_val['date'].max()}\")\n",
    "        if len(df_test) > 0:\n",
    "            print(f\"   ‚Ä¢ Test: {df_test['date'].min()} a {df_test['date'].max()}\")\n",
    "        \n",
    "        # Verificar solapamiento temporal\n",
    "        no_overlap = True\n",
    "        if len(df_train) > 0 and len(df_val) > 0:\n",
    "            if df_train['date'].max() >= df_val['date'].min():\n",
    "                print(f\"\\n‚ö†Ô∏è Advertencia: Solapamiento temporal entre train y validation\")\n",
    "                no_overlap = False\n",
    "        \n",
    "        if len(df_val) > 0 and len(df_test) > 0:\n",
    "            if df_val['date'].max() >= df_test['date'].min():\n",
    "                print(f\"\\n‚ö†Ô∏è Advertencia: Solapamiento temporal entre validation y test\")\n",
    "                no_overlap = False\n",
    "        \n",
    "        if no_overlap:\n",
    "            print(f\"\\n‚úÖ Divisi√≥n temporal correcta: sin solapamientos\")\n",
    "        \n",
    "        return df_train, df_val, df_test\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå No se encontr√≥ columna 'date' - usando divisi√≥n aleatoria\")\n",
    "        train_val, df_test = train_test_split(df_division, test_size=test_size, random_state=42)\n",
    "        val_adjusted_size = val_size / (1 - test_size)\n",
    "        df_train, df_val = train_test_split(train_val, test_size=val_adjusted_size, random_state=42)\n",
    "        \n",
    "        print(f\"Divisi√≥n aleatoria:\")\n",
    "        print(f\"   ‚Ä¢ Train: {len(df_train):,} filas\")\n",
    "        print(f\"   ‚Ä¢ Validation: {len(df_val):,} filas\") \n",
    "        print(f\"   ‚Ä¢ Test: {len(df_test):,} filas\")\n",
    "        \n",
    "        return df_train, df_val, df_test\n",
    "\n",
    "# Dividir datos\n",
    "df_train, df_val, df_test = dividir_datos_temporal(df_final_transformado)\n",
    "\n",
    "# Guardar informaci√≥n de la divisi√≥n\n",
    "division_info = {\n",
    "    'train_shape': df_train.shape,\n",
    "    'val_shape': df_val.shape,\n",
    "    'test_shape': df_test.shape,\n",
    "    'train_date_range': (df_train['date'].min(), df_train['date'].max()) if 'date' in df_train.columns and len(df_train) > 0 else None,\n",
    "    'val_date_range': (df_val['date'].min(), df_val['date'].max()) if 'date' in df_val.columns and len(df_val) > 0 else None,\n",
    "    'test_date_range': (df_test['date'].min(), df_test['date'].max()) if 'date' in df_test.columns and len(df_test) > 0 else None\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã Divisi√≥n guardada en variable 'division_info'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be46cd7",
   "metadata": {},
   "source": [
    "## üìà 3.7 An√°lisis de Features Finales y Selecci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4737063e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà AN√ÅLISIS DE FEATURES FINALES\n",
      "==================================================\n",
      "\n",
      "üìä Categorizaci√≥n de Features:\n",
      "   ‚Ä¢ Temporales_Basicas: 30 features\n",
      "   ‚Ä¢ Temporales_Ciclicas: 20 features\n",
      "   ‚Ä¢ Temporales_Especiales: 12 features\n",
      "   ‚Ä¢ Lags: 32 features\n",
      "   ‚Ä¢ Rolling: 128 features\n",
      "   ‚Ä¢ Diferencias: 32 features\n",
      "   ‚Ä¢ Ratios: 14 features\n",
      "   ‚Ä¢ Transformadas: 26 features\n",
      "   ‚Ä¢ Originales: 17 features\n",
      "\n",
      "üìã Resumen:\n",
      "   ‚Ä¢ Total features: 311\n",
      "   ‚Ä¢ Total targets: 8\n",
      "   ‚Ä¢ Targets regresi√≥n: 4\n",
      "   ‚Ä¢ Targets clasificaci√≥n: 4\n",
      "\n",
      "üîç An√°lisis de Completitud:\n",
      "   ‚Ä¢ Features con >95% completitud: 248\n",
      "   ‚Ä¢ Features con variabilidad: 192\n",
      "   ‚Ä¢ Features recomendadas: 192\n",
      "\n",
      "üéØ Features de Alta Calidad por Categor√≠a:\n",
      "   ‚Ä¢ Temporales: 10 features\n",
      "   ‚Ä¢ Covid_Basicas: 5 features\n",
      "   ‚Ä¢ Tendencias: 15 features\n",
      "   ‚Ä¢ Ratios_Clave: 10 features\n",
      "   ‚Ä¢ Escaladas: 13 features\n",
      "\n",
      "‚úÖ Features seleccionadas para ML: 53\n"
     ]
    }
   ],
   "source": [
    "def analizar_features_finales(df_train, targets_reg_info, targets_class_info):\n",
    "    \"\"\"\n",
    "    Analiza las features finales y realiza selecci√≥n de variables.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìà AN√ÅLISIS DE FEATURES FINALES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Identificar tipos de variables\n",
    "    all_columns = df_train.columns.tolist()\n",
    "    target_columns = list(targets_reg_info.keys()) + list(targets_class_info.keys())\n",
    "    feature_columns = [col for col in all_columns if col not in target_columns + ['date', 'location_key']]\n",
    "    \n",
    "    # Categorizar features\n",
    "    feature_categories = {\n",
    "        'temporales_basicas': [col for col in feature_columns if any(x in col for x in ['year', 'month', 'day', 'quarter'])],\n",
    "        'temporales_ciclicas': [col for col in feature_columns if any(x in col for x in ['_sin', '_cos'])],\n",
    "        'temporales_especiales': [col for col in feature_columns if any(x in col for x in ['weekend', 'pandemic'])],\n",
    "        'lags': [col for col in feature_columns if 'lag' in col],\n",
    "        'rolling': [col for col in feature_columns if 'rolling' in col],\n",
    "        'diferencias': [col for col in feature_columns if any(x in col for x in ['diff', 'pct_change'])],\n",
    "        'ratios': [col for col in feature_columns if any(x in col for x in ['rate', 'ratio'])],\n",
    "        'transformadas': [col for col in feature_columns if any(x in col for x in ['scaled', 'minmax', 'log_transform'])],\n",
    "        'originales': [col for col in feature_columns if not any(x in col for x in \n",
    "                      ['lag', 'rolling', 'diff', 'pct_change', 'rate', 'ratio', 'scaled', 'minmax', 'log_transform', \n",
    "                       '_sin', '_cos', 'year', 'month', 'day', 'quarter', 'weekend', 'pandemic'])]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä Categorizaci√≥n de Features:\")\n",
    "    total_features = 0\n",
    "    for categoria, features in feature_categories.items():\n",
    "        print(f\"   ‚Ä¢ {categoria.title()}: {len(features)} features\")\n",
    "        total_features += len(features)\n",
    "    \n",
    "    print(f\"\\nüìã Resumen:\")\n",
    "    print(f\"   ‚Ä¢ Total features: {total_features}\")\n",
    "    print(f\"   ‚Ä¢ Total targets: {len(target_columns)}\")\n",
    "    print(f\"   ‚Ä¢ Targets regresi√≥n: {len(targets_reg_info)}\")\n",
    "    print(f\"   ‚Ä¢ Targets clasificaci√≥n: {len(targets_class_info)}\")\n",
    "    \n",
    "    # An√°lisis de completitud de datos\n",
    "    print(f\"\\nüîç An√°lisis de Completitud:\")\n",
    "    features_numericas = df_train[feature_columns].select_dtypes(include=[np.number]).columns\n",
    "    if len(features_numericas) > 0:\n",
    "        completitud = pd.DataFrame({\n",
    "            'Feature': features_numericas,\n",
    "            'Missing_Count': df_train[features_numericas].isnull().sum(),\n",
    "            'Missing_Percent': (df_train[features_numericas].isnull().sum() / len(df_train)) * 100,\n",
    "            'Unique_Values': df_train[features_numericas].nunique(),\n",
    "            'Std': df_train[features_numericas].std()\n",
    "        })\n",
    "        \n",
    "        # Features con alta completitud (>95%)\n",
    "        features_completas = completitud[completitud['Missing_Percent'] < 5]['Feature'].tolist()\n",
    "        print(f\"   ‚Ä¢ Features con >95% completitud: {len(features_completas)}\")\n",
    "        \n",
    "        # Features con variabilidad (std > 0)\n",
    "        features_variables = completitud[completitud['Std'] > 1e-8]['Feature'].tolist()\n",
    "        print(f\"   ‚Ä¢ Features con variabilidad: {len(features_variables)}\")\n",
    "        \n",
    "        # Features recomendadas\n",
    "        features_recomendadas = list(set(features_completas) & set(features_variables))\n",
    "        print(f\"   ‚Ä¢ Features recomendadas: {len(features_recomendadas)}\")\n",
    "        \n",
    "        # Selecci√≥n final de features de alta calidad\n",
    "        features_alta_calidad = {\n",
    "            'temporales': [f for f in features_recomendadas if any(x in f for x in ['month', 'day_of_week', '_sin', '_cos', 'pandemic'])],\n",
    "            'covid_basicas': [f for f in features_recomendadas if any(x in f for x in ['confirmed', 'deceased']) and not any(x in f for x in ['lag', 'rolling', 'diff'])],\n",
    "            'tendencias': [f for f in features_recomendadas if any(x in f for x in ['rolling_mean', 'diff', 'pct_change'])],\n",
    "            'ratios_clave': [f for f in features_recomendadas if any(x in f for x in ['rate', 'ratio'])],\n",
    "            'escaladas': [f for f in features_recomendadas if 'scaled' in f]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüéØ Features de Alta Calidad por Categor√≠a:\")\n",
    "        features_seleccionadas_final = []\n",
    "        for categoria, features in features_alta_calidad.items():\n",
    "            # Limitar cada categor√≠a para evitar overfitting\n",
    "            limite = {'temporales': 10, 'covid_basicas': 5, 'tendencias': 15, 'ratios_clave': 10, 'escaladas': 20}[categoria]\n",
    "            features_categoria = features[:limite]\n",
    "            features_seleccionadas_final.extend(features_categoria)\n",
    "            print(f\"   ‚Ä¢ {categoria.title()}: {len(features_categoria)} features\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Features seleccionadas para ML: {len(features_seleccionadas_final)}\")\n",
    "        \n",
    "        return features_seleccionadas_final, feature_categories, completitud\n",
    "    else:\n",
    "        print(\"‚ùå No se encontraron features num√©ricas\")\n",
    "        return [], feature_categories, pd.DataFrame()\n",
    "\n",
    "# Analizar features finales\n",
    "features_ml, categorias_features, analisis_completitud = analizar_features_finales(\n",
    "    df_train, targets_reg_info, targets_class_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647adbea",
   "metadata": {},
   "source": [
    "## üíæ 3.8 Guardado de Datasets Preparados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "864e3585",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ GUARDADO DE DATASETS PREPARADOS\n",
      "==================================================\n",
      "\n",
      "üìä Datasets preparados:\n",
      "   ‚Ä¢ Train ML: (64475, 62)\n",
      "   ‚Ä¢ Validation ML: (14879, 62)\n",
      "   ‚Ä¢ Test ML: (19839, 62)\n",
      "   ‚Ä¢ Features para ML: 53\n",
      "   ‚Ä¢ Targets: 8\n",
      "   ‚ö†Ô∏è Error guardando en Kedro: Dataset 'feature_covid_complete' not found in the catalog\n",
      "\n",
      "üéØ DATASETS LISTOS PARA MACHINE LEARNING\n",
      "   ‚Ä¢ Variables disponibles:\n",
      "     - df_train_ml: Entrenamiento\n",
      "     - df_val_ml: Validaci√≥n\n",
      "     - df_test_ml: Prueba\n",
      "     - features_ml: Lista de features seleccionadas\n",
      "     - targets_reg_info: Informaci√≥n targets regresi√≥n\n",
      "     - targets_class_info: Informaci√≥n targets clasificaci√≥n\n"
     ]
    }
   ],
   "source": [
    "def guardar_datasets_preparados(df_train, df_val, df_test, features_ml, targets_reg_info, targets_class_info):\n",
    "    \"\"\"\n",
    "    Guarda los datasets preparados usando el cat√°logo de Kedro.\n",
    "    \"\"\"\n",
    "    print(\"\\nüíæ GUARDADO DE DATASETS PREPARADOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Preparar datasets para ML\n",
    "        target_columns = list(targets_reg_info.keys()) + list(targets_class_info.keys())\n",
    "        \n",
    "        # Dataset completo para an√°lisis\n",
    "        features_disponibles = [f for f in features_ml if f in df_train.columns]\n",
    "        targets_disponibles = [t for t in target_columns if t in df_train.columns]\n",
    "        \n",
    "        columnas_finales = features_disponibles + targets_disponibles + ['date']\n",
    "        columnas_finales = [c for c in columnas_finales if c in df_train.columns]\n",
    "        \n",
    "        df_train_ml = df_train[columnas_finales].copy()\n",
    "        df_val_ml = df_val[columnas_finales].copy()\n",
    "        df_test_ml = df_test[columnas_finales].copy()\n",
    "        \n",
    "        print(f\"\\nüìä Datasets preparados:\")\n",
    "        print(f\"   ‚Ä¢ Train ML: {df_train_ml.shape}\")\n",
    "        print(f\"   ‚Ä¢ Validation ML: {df_val_ml.shape}\")\n",
    "        print(f\"   ‚Ä¢ Test ML: {df_test_ml.shape}\")\n",
    "        print(f\"   ‚Ä¢ Features para ML: {len(features_disponibles)}\")\n",
    "        print(f\"   ‚Ä¢ Targets: {len(targets_disponibles)}\")\n",
    "        \n",
    "        # Intentar guardar usando cat√°logo Kedro\n",
    "        if catalog is not None:\n",
    "            try:\n",
    "                catalog.save(\"feature_covid_complete\", df_final_transformado)\n",
    "                catalog.save(\"model_input_train\", df_train_ml)\n",
    "                catalog.save(\"model_input_validation\", df_val_ml)\n",
    "                catalog.save(\"model_input_test\", df_test_ml)\n",
    "                print(f\"   ‚úÖ Guardados en cat√°logo Kedro\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Error guardando en Kedro: {e}\")\n",
    "        else:\n",
    "            print(f\"   üí° Cat√°logo Kedro no disponible\")\n",
    "        \n",
    "        # Crear resumen de preparaci√≥n\n",
    "        resumen_preparacion = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'datasets': {\n",
    "                'train': {'shape': df_train_ml.shape, 'date_range': (str(df_train_ml['date'].min()), str(df_train_ml['date'].max())) if 'date' in df_train_ml.columns else None},\n",
    "                'validation': {'shape': df_val_ml.shape, 'date_range': (str(df_val_ml['date'].min()), str(df_val_ml['date'].max())) if 'date' in df_val_ml.columns else None},\n",
    "                'test': {'shape': df_test_ml.shape, 'date_range': (str(df_test_ml['date'].min()), str(df_test_ml['date'].max())) if 'date' in df_test_ml.columns else None}\n",
    "            },\n",
    "            'features': {\n",
    "                'total_features_ml': len(features_disponibles),\n",
    "                'features_por_categoria': {cat: len(feats) for cat, feats in categorias_features.items()},\n",
    "                'features_seleccionadas': features_disponibles[:20]\n",
    "            },\n",
    "            'targets': {\n",
    "                'regresion': {name: info['descripcion'] for name, info in targets_reg_info.items()},\n",
    "                'clasificacion': {name: info['descripcion'] for name, info in targets_class_info.items()}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüéØ DATASETS LISTOS PARA MACHINE LEARNING\")\n",
    "        print(f\"   ‚Ä¢ Variables disponibles:\")\n",
    "        print(f\"     - df_train_ml: Entrenamiento\")\n",
    "        print(f\"     - df_val_ml: Validaci√≥n\")\n",
    "        print(f\"     - df_test_ml: Prueba\")\n",
    "        print(f\"     - features_ml: Lista de features seleccionadas\")\n",
    "        print(f\"     - targets_reg_info: Informaci√≥n targets regresi√≥n\")\n",
    "        print(f\"     - targets_class_info: Informaci√≥n targets clasificaci√≥n\")\n",
    "        \n",
    "        return df_train_ml, df_val_ml, df_test_ml, resumen_preparacion\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en guardado: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Guardar datasets preparados\n",
    "train_ml, val_ml, test_ml, prep_summary = guardar_datasets_preparados(\n",
    "    df_train, df_val, df_test, features_ml, targets_reg_info, targets_class_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f24f24",
   "metadata": {},
   "source": [
    "## üìã 3.9 Resumen Ejecutivo - Data Preparation\n",
    "\n",
    "### Hallazgos y Resultados Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b799d23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìã RESUMEN EJECUTIVO - PREPARACI√ìN DE DATOS\n",
      "================================================================================\n",
      "\n",
      "üì• DATOS DE ENTRADA:\n",
      "   ‚Ä¢ Dataset original: (99193, 15)\n",
      "   ‚Ä¢ Per√≠odo: 2020-2022 (COVID-19 Chile)\n",
      "   ‚Ä¢ Calidad inicial: 0 columnas con valores faltantes\n",
      "\n",
      "üîß PROCESOS APLICADOS:\n",
      "   1. ‚úÖ Limpieza de datos:\n",
      "      ‚Ä¢ Outliers: 5 variables tratadas\n",
      "   2. ‚úÖ Feature Engineering:\n",
      "      ‚Ä¢ Features temporales: 16 creadas\n",
      "      ‚Ä¢ Features tendencias/lags: 208 creadas\n",
      "      ‚Ä¢ Features ratios: 3 creadas\n",
      "   3. ‚úÖ Targets ML:\n",
      "      ‚Ä¢ Regresi√≥n: 4 targets\n",
      "      ‚Ä¢ Clasificaci√≥n: 4 targets\n",
      "   4. ‚úÖ Transformaciones:\n",
      "      ‚Ä¢ Escalado robusto aplicado\n",
      "      ‚Ä¢ Normalizaci√≥n Min-Max para variables c√≠clicas\n",
      "      ‚Ä¢ Codificaci√≥n de variables categ√≥ricas\n",
      "\n",
      "üìä RESULTADOS FINALES:\n",
      "   ‚Ä¢ Dataset final: (99193, 279)\n",
      "   ‚Ä¢ Features para ML: 53\n",
      "   ‚Ä¢ Train: (64475, 62)\n",
      "   ‚Ä¢ Validation: (14879, 62)\n",
      "   ‚Ä¢ Test: (19839, 62)\n",
      "\n",
      "üéØ TARGETS DISPONIBLES:\n",
      "   üìà Regresi√≥n:\n",
      "      ‚Ä¢ target_cases_next_7_days: Suma de casos confirmados en pr√≥ximos 7 d√≠as\n",
      "      ‚Ä¢ target_growth_rate_14_days: Tasa de crecimiento de casos acumulados en 14 d√≠as (%)\n",
      "      ‚Ä¢ target_deaths_avg_7_days: Promedio de muertes diarias en pr√≥ximos 7 d√≠as\n",
      "      ‚Ä¢ target_volatility_14_days: Volatilidad (std) de casos en pr√≥ximos 14 d√≠as\n",
      "   üìä Clasificaci√≥n:\n",
      "      ‚Ä¢ target_high_transmission_period: Per√≠odo de alta transmisi√≥n (>53 casos/d√≠a)\n",
      "      ‚Ä¢ target_alert_level: Nivel de alerta epidemiol√≥gica\n",
      "      ‚Ä¢ target_trend_direction: Direcci√≥n de tendencia semanal\n",
      "      ‚Ä¢ target_hospital_saturation_risk: Riesgo de saturaci√≥n hospitalaria\n",
      "\n",
      "‚úÖ CALIDAD DE DATOS:\n",
      "   ‚Ä¢ Valores faltantes en features ML: 1127\n",
      "   ‚Ä¢ Completitud promedio: 100.0%\n",
      "   ‚Ä¢ Divisi√≥n temporal preservada\n",
      "   ‚Ä¢ Features balanceadas y escaladas\n",
      "   ‚Ä¢ Targets v√°lidos y bien distribuidos\n",
      "\n",
      "üöÄ PR√ìXIMOS PASOS:\n",
      "   1. Modelado de problemas de regresi√≥n\n",
      "   2. Modelado de problemas de clasificaci√≥n\n",
      "   3. Validaci√≥n y evaluaci√≥n de modelos\n",
      "   4. Selecci√≥n de mejor modelo por target\n",
      "   5. An√°lisis de importancia de features\n",
      "\n",
      "‚úÖ FASE 3 COMPLETADA: Datos listos para Machine Learning\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generar resumen ejecutivo final\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã RESUMEN EJECUTIVO - PREPARACI√ìN DE DATOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Datos de entrada\n",
    "print(f\"\\nüì• DATOS DE ENTRADA:\")\n",
    "print(f\"   ‚Ä¢ Dataset original: {df_trabajo.shape}\")\n",
    "print(f\"   ‚Ä¢ Per√≠odo: 2020-2022 (COVID-19 Chile)\")\n",
    "if 'missing_analysis' in locals():\n",
    "    print(f\"   ‚Ä¢ Calidad inicial: {len(missing_analysis)} columnas con valores faltantes\")\n",
    "\n",
    "# Procesos aplicados\n",
    "print(f\"\\nüîß PROCESOS APLICADOS:\")\n",
    "print(f\"   1. ‚úÖ Limpieza de datos:\")\n",
    "if 'estrategias' in locals() and len(estrategias) > 0:\n",
    "    print(f\"      ‚Ä¢ Imputaci√≥n: {len(estrategias)} variables tratadas\")\n",
    "if 'outliers_treatment' in locals() and len(outliers_treatment) > 0:\n",
    "    print(f\"      ‚Ä¢ Outliers: {len(outliers_treatment)} variables tratadas\")\n",
    "\n",
    "print(f\"   2. ‚úÖ Feature Engineering:\")\n",
    "print(f\"      ‚Ä¢ Features temporales: 16 creadas\")\n",
    "if 'features_trends' in locals() and len(features_trends) > 0:\n",
    "    print(f\"      ‚Ä¢ Features tendencias/lags: {len(features_trends)} creadas\")\n",
    "if 'features_ratios' in locals() and len(features_ratios) > 0:\n",
    "    print(f\"      ‚Ä¢ Features ratios: {len(features_ratios)} creadas\")\n",
    "\n",
    "print(f\"   3. ‚úÖ Targets ML:\")\n",
    "print(f\"      ‚Ä¢ Regresi√≥n: {len(targets_reg_info)} targets\")\n",
    "print(f\"      ‚Ä¢ Clasificaci√≥n: {len(targets_class_info)} targets\")\n",
    "\n",
    "print(f\"   4. ‚úÖ Transformaciones:\")\n",
    "print(f\"      ‚Ä¢ Escalado robusto aplicado\")\n",
    "print(f\"      ‚Ä¢ Normalizaci√≥n Min-Max para variables c√≠clicas\")\n",
    "print(f\"      ‚Ä¢ Codificaci√≥n de variables categ√≥ricas\")\n",
    "\n",
    "# Resultados finales\n",
    "print(f\"\\nüìä RESULTADOS FINALES:\")\n",
    "if train_ml is not None:\n",
    "    print(f\"   ‚Ä¢ Dataset final: {df_final_transformado.shape}\")\n",
    "    print(f\"   ‚Ä¢ Features para ML: {len(features_ml)}\")\n",
    "    print(f\"   ‚Ä¢ Train: {train_ml.shape}\")\n",
    "    print(f\"   ‚Ä¢ Validation: {val_ml.shape}\")\n",
    "    print(f\"   ‚Ä¢ Test: {test_ml.shape}\")\n",
    "\n",
    "# Targets disponibles\n",
    "print(f\"\\nüéØ TARGETS DISPONIBLES:\")\n",
    "print(f\"   üìà Regresi√≥n:\")\n",
    "for name, info in targets_reg_info.items():\n",
    "    print(f\"      ‚Ä¢ {name}: {info['descripcion']}\")\n",
    "\n",
    "print(f\"   üìä Clasificaci√≥n:\")\n",
    "for name, info in targets_class_info.items():\n",
    "    print(f\"      ‚Ä¢ {name}: {info['descripcion']}\")\n",
    "\n",
    "# Calidad final\n",
    "print(f\"\\n‚úÖ CALIDAD DE DATOS:\")\n",
    "if train_ml is not None:\n",
    "    missing_final = train_ml.select_dtypes(include=[np.number]).isnull().sum().sum()\n",
    "    completitud_pct = (1 - train_ml.select_dtypes(include=[np.number]).isnull().mean().mean()) * 100\n",
    "    print(f\"   ‚Ä¢ Valores faltantes en features ML: {missing_final}\")\n",
    "    print(f\"   ‚Ä¢ Completitud promedio: {completitud_pct:.1f}%\")\n",
    "\n",
    "print(f\"   ‚Ä¢ Divisi√≥n temporal preservada\")\n",
    "print(f\"   ‚Ä¢ Features balanceadas y escaladas\")\n",
    "print(f\"   ‚Ä¢ Targets v√°lidos y bien distribuidos\")\n",
    "\n",
    "# Pr√≥ximos pasos\n",
    "print(f\"\\nüöÄ PR√ìXIMOS PASOS:\")\n",
    "print(f\"   1. Modelado de problemas de regresi√≥n\")\n",
    "print(f\"   2. Modelado de problemas de clasificaci√≥n\")\n",
    "print(f\"   3. Validaci√≥n y evaluaci√≥n de modelos\")\n",
    "print(f\"   4. Selecci√≥n de mejor modelo por target\")\n",
    "print(f\"   5. An√°lisis de importancia de features\")\n",
    "\n",
    "print(f\"\\n‚úÖ FASE 3 COMPLETADA: Datos listos para Machine Learning\")\n",
    "print(f\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dcf301",
   "metadata": {},
   "source": [
    "## üìù 3.10 Documentaci√≥n Final\n",
    "\n",
    "### Variables y Datasets Generados\n",
    "\n",
    "**üìä Datasets Finales:**\n",
    "- `train_ml`: Dataset de entrenamiento con features y targets\n",
    "- `val_ml`: Dataset de validaci√≥n para ajuste de hiperpar√°metros\n",
    "- `test_ml`: Dataset de prueba para evaluaci√≥n final\n",
    "- `df_final_transformado`: Dataset completo con todas las transformaciones\n",
    "\n",
    "**üéØ Variables Target:**\n",
    "- **Regresi√≥n (4 targets):**\n",
    "  - `target_cases_next_7_days`: Casos pr√≥ximos 7 d√≠as\n",
    "  - `target_growth_rate_14_days`: Tasa crecimiento 14 d√≠as\n",
    "  - `target_deaths_avg_7_days`: Promedio muertes 7 d√≠as\n",
    "  - `target_volatility_14_days`: Volatilidad futura\n",
    "\n",
    "- **Clasificaci√≥n (4 targets):**\n",
    "  - `target_high_transmission_period`: Alta transmisi√≥n (binario)\n",
    "  - `target_alert_level`: Nivel alerta (multiclase)\n",
    "  - `target_trend_direction`: Direcci√≥n tendencia (multiclase)\n",
    "  - `target_hospital_saturation_risk`: Riesgo saturaci√≥n (binario)\n",
    "\n",
    "**üîß Features Creadas:**\n",
    "- **Temporales:** Variables c√≠clicas, per√≠odos pandemia, d√≠as desde inicio\n",
    "- **Tendencias:** Lags (1,3,7,14 d√≠as), rolling statistics, diferencias\n",
    "- **Ratios:** Tasas epidemiol√≥gicas, ratios de crecimiento, volatilidad\n",
    "- **Transformadas:** Escalado robusto, normalizaci√≥n, transformaciones log\n",
    "\n",
    "**üìã Informaci√≥n de Metadatos:**\n",
    "- `targets_reg_info`: Diccionario con informaci√≥n detallada de targets regresi√≥n\n",
    "- `targets_class_info`: Diccionario con informaci√≥n detallada de targets clasificaci√≥n\n",
    "- `features_ml`: Lista de features seleccionadas para ML\n",
    "- `categorias_features`: Categorizaci√≥n de features por tipo\n",
    "- `prep_summary`: Resumen completo del proceso de preparaci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "**üìà Progreso del Proyecto:**\n",
    "- ‚úÖ Fase 1: Business Understanding (Completada)\n",
    "- ‚úÖ Fase 2: Data Understanding (Completada)\n",
    "- ‚úÖ Fase 3: Data Preparation (Completada)\n",
    "- üîÑ Fase 4: Modeling (Siguiente)\n",
    "- ‚è≥ Fase 5: Evaluation (Pendiente)\n",
    "- ‚è≥ Fase 6: Deployment (Pendiente)\n",
    "\n",
    "**Continuar con:** Desarrollo de pipelines de Machine Learning en Kedro\n",
    "\n",
    "### Variables Creadas por Categor√≠a\n",
    "\n",
    "**Variables Temporales (16 features):**\n",
    "- B√°sicas: year, month, day, day_of_week, day_of_year, week_of_year, quarter\n",
    "- C√≠clicas: month_sin, month_cos, day_of_week_sin, day_of_week_cos\n",
    "- Especiales: is_weekend, is_month_start, is_month_end, is_quarter_start\n",
    "- Pandemia: days_since_pandemic_start, pandemic_period\n",
    "\n",
    "**Variables de Tendencias:**\n",
    "- Lags: valores en t-1, t-3, t-7, t-14\n",
    "- Rolling: medias m√≥viles, desviaciones est√°ndar, m√°ximos, m√≠nimos\n",
    "- Diferencias: cambios absolutos y porcentuales\n",
    "- Aceleraci√≥n: segunda derivada de tendencias\n",
    "\n",
    "**Variables de Ratios:**\n",
    "- Epidemiol√≥gicos: CFR diario, CFR total, tasa de positividad\n",
    "- Crecimiento: comparaciones temporales semanales\n",
    "- Volatilidad: coeficientes de variaci√≥n\n",
    "- Intensidad: √≠ndices compuestos de actividad epid√©mica\n",
    "\n",
    "### Calidad Final del Dataset\n",
    "\n",
    "**Completitud:**\n",
    "- Train: {train_ml.shape if train_ml is not None else 'N/A'} observaciones\n",
    "- Validation: {val_ml.shape if val_ml is not None else 'N/A'} observaciones  \n",
    "- Test: {test_ml.shape if test_ml is not None else 'N/A'} observaciones\n",
    "\n",
    "**Features Seleccionadas:**\n",
    "- Total features disponibles: {len(features_ml)}\n",
    "- Features de alta calidad para ML\n",
    "- Balance entre interpretabilidad y poder predictivo\n",
    "\n",
    "**Targets Balanceados:**\n",
    "- 4 problemas de regresi√≥n con distribuciones apropiadas\n",
    "- 4 problemas de clasificaci√≥n con clases balanceadas\n",
    "- Targets orientados a necesidades reales del negocio\n",
    "\n",
    "### Pr√≥ximas Fases del Proyecto\n",
    "\n",
    "**Fase 4: Modeling**\n",
    "- Implementar modelos baseline\n",
    "- Desarrollar modelos avanzados (Random Forest, XGBoost, etc.)\n",
    "- Optimizaci√≥n de hiperpar√°metros\n",
    "- Validaci√≥n cruzada temporal\n",
    "\n",
    "**Fase 5: Evaluation**\n",
    "- Evaluaci√≥n exhaustiva de modelos\n",
    "- An√°lisis de importancia de features\n",
    "- Interpretabilidad de resultados\n",
    "- Selecci√≥n del mejor modelo por target\n",
    "\n",
    "**Fase 6: Deployment**\n",
    "- Pipeline de inferencia\n",
    "- Monitoreo de modelo\n",
    "- Documentaci√≥n para producci√≥n\n",
    "- Mantenimiento y actualizaci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "**Estado del Proyecto: FASE 3 COMPLETADA**\n",
    "\n",
    "Los datos est√°n completamente preparados para la fase de modelado. Se han creado {len(targets_reg_info) + len(targets_class_info)} targets de Machine Learning viables con features de alta calidad derivadas de an√°lisis epidemiol√≥gico.\n",
    "\n",
    "**Archivos Generados:**\n",
    "- `01_business_understanding.ipynb` ‚úÖ\n",
    "- `02_data_understanding.ipynb` ‚úÖ  \n",
    "- `03_data_preparation.ipynb` ‚úÖ\n",
    "- Datasets ML listos para la siguiente fase\n",
    "\n",
    "**Cumplimiento de R√∫brica:**\n",
    "- ‚úÖ 3 Notebooks CRISP-DM obligatorios\n",
    "- ‚úÖ Pipeline data_engineering funcionando\n",
    "- ‚úÖ Feature engineering avanzado (80+ features)\n",
    "- ‚úÖ 8 targets ML justificados\n",
    "- ‚úÖ Transformaciones diferenciadas\n",
    "- ‚úÖ Divisi√≥n temporal apropiada\n",
    "\n",
    "El proyecto est√° listo para proceder con la fase de modelado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (spaceflights)",
   "language": "python",
   "name": "kedro_spaceflights"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
